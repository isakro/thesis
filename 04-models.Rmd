# Model-based archaeology

Over the years, several works have purported the benefits of a model-based archaeology [@clarke1972; @wylie2002, 91--96], which has especially gained a footing within the sub-field of computational archaeology [e.g. @kohler2007; @lake2015; @romanowska2015; @brughmans2021]. The goal of the next two chapters is twofold. First to elucidate what defines or can define a model-based scientific approach, and in the next chapter to demonstrate how this can form a useful framework for archaeological inquiry by drawing on examples from the papers of this thesis. Central to the following sections are four problem areas in the understanding scientific models, as identified by @frigg2018[1]: 1) The ontological: what are models? 2) The semantic: what do models represent? 3) The epistemological: how do we learn with models? And 4) what consequences do the use of models have for overarching principles such as scientific realism, reductionism and explanation?

One fairly common understanding of models simply entail seeing them as a set of simplifications or assumptions concerning real-world phenomena [e.g. @barton2013, 154]. Any representation could thus be considered a model whether it is generated physically, digitally, verbally, simply imagined, or is construed in a natural or formal language. Scholars arguing the case for model-based archaeology often start out by making the point that whether we acknowledge it or not, we always employ such abstractions when attempting to understand past reality [@kohler2007, 4; @lake2015, 7]. The infinite complexity of reality means that any description of it has to be a simplification, and even if we were able to, a complete rendition of reality would not be a worthwhile endeavour in its own right. A perfect reconstruction of reality would be a tautology, which without perspective offers neither insight nor understanding [@yarrow2006, 77; @slingerland2012, 14--19]. Put differently, whether we understand archaeology as tasked with providing explanation, understanding, or interesting narratives about the past, any demand for a higher empirical resolution, for its own sake, would be a refutation of theory [see @healy2017]. These are, however, universal scientific points, variations of which have been made under diverse headings of archaeological theory [e.g. @johnson2010, 7]. It would thus follow that if the term model is taken to denote all generalisations or abstractions of reality, which in its ubiquity would include any description or explanation, it is not given why this would have to be dealt with within a comprehensive model-based archaeology. The arguments in favour of a distinct model-based archaeology tend to follow from *how* this necessary simplification should be made, and in turn handled. What this entails can be foreshadowed here by invoking the classic quote from @box1979[2]: \'All models are wrong but some are useful\'. But if all models are wrong, what is their epistemic value? To begin to answer this question the above view of models, simply seeing them as abstractions, will be accepted for now without regard for their demarcation to data, theory and hypotheses.

## Confronting beliefs with data

The explicit testing of archaeological explanations was assertively introduced to the discipline with processualism, which argued that archaeology should adopt the explanatory goals of positivist social sciences. How this was to be done first follows from the standard processual view on what the archaeological material represents. Here, material culture was seen as an integrated part of -- and the result of -- total, multidimensional cultural systems [e.g. @binford1962]. As such, theories concerning how all aspects of cultural systems would influence and manifest in the material record should be conceived. Central to this is that the archaeological material represents an objective, albeit complex empirical record that reflects empirical causes, irrespective of our beliefs about what these causes are. The empirical material will in this processual understanding therefore offer a direct link back to this systemic whole. Archaeological material is therefore representative of the multidimensional causal chain from cultural system to the archaeological record. Theories concerning the prehistoric systemic whole and what processes have influenced the remnants available to us must therefore come prior to an archaeological investigation for it to be possible to evaluate their veracity. As complex integrated wholes, such models of entire systems were then to be tested by drawing on the hypothetico-deductive approach. Furthermore, drawing on the covering-law framework as taken from Hempel's logical positivism/empiricism, the ultimate goal was to establish laws pertaining to the conjunct occurrence of certain types of material remains with certain types of societal systems, irrespective of time and place.

It should be noted here that the programme of logical positivism, and the more mature logical empiricism, were far more nuanced that what they are often given credit for in the archaeological literature concerned with establishing why these views were misguided [see @gibbon1989, 8--60]. This is equally true for the over-simplifying presentation that is given here. However, this can in part be justified with reference to the naive versions of these programmes that were adopted by positivist social science and archaeology at the time [@gibbon1989, 91--117].

According to Hempel, the goal of science is to establish laws that are deductively valid, of the kind given by the classic example *All men are mortal \/ Socrates is a man \/ Therefore, Socrates is mortal*. If the premises are true, then the conclusion will always be true. However, when adapted to archaeology, the proposed laws were so banal that @flannery1973[] stated that attempts at adopting Hempelian empiricism \'has produced some of the worst archaeology on record\'. The search for covering laws was therefore quickly abandoned by most practitioners. Furthermore, whether an argument is deductively valid or not is not dependent on whether or not the premises are true. If it happens to be true that all men are mortal and Socrates is a man, then the deductively valid argument is said to be sound. Determining whether the premises are true depends on non-deductive reasoning. A deductively derived test that successfully corresponds with data only supports the hypothesis inductively [@chapman2016, 27]. However, giving up on the search for covering laws and deductive certainty need not entail that hypothetico-deductive testing is misguided, and more modest goals of confronting beliefs pertaining to specific contexts or research questions with data is, I will argue, still very much a viable goal.

Smith [-@smith2015; @smith2017] has stated that one of the most central questions we can ask about our archaeological explanations is \'How would you know if you are wrong?\'. Archaeological explanation often take the form of what @binford1981 termed a *post hoc* accommodative argument. This involves first gathering and categorising the data of interest, often using variables chosen by convention and convenience, and then building an explanation around any discerned patterns [@clark2009, 29]. This inductive data-dredging or pattern-searching approach is argued to constitute a limited inferential framework for a couple of reasons. 

First, what among the virtually infinite aspects of the material available to us is considered interesting will always be determined by our beliefs concerning the processes that have resulted in their manifestation. What characteristics of the material is recorded and drawn on to organise it will dictate what patterns one can hope to reveal. As @popper1989[46] framed it, without an underlying theory, how would we know what to look for? If one follows what has been done conventionally, without taking any explicit stance towards this, one will be dependent on how others have conceived of what questions are of interest and how these can be answered. Furthermore, at no point in this process can our argument be falsified, and @smith2015[19] likens this exercise with \'the farmer who paints bulls-eyes around the bullet holes in his barn in order to show his superior shooting skills.\' *Post hoc* accommodative arguments can provide the identification of empirical patterns with respects to the employed units of analysis, which in turn can form the basis for social and behavioural hypotheses. But @binford1981[85] has argued that such arguments can at best be \'treated as provocative ideas in need of evaluation\'. @clark2009[29] states that a necessary next step is to derive empirical implications of this hypothesis, which can be evaluated against a part of the archaeological record that is independent from the material originally used to derive it [also @barton2013]. Subsequent testing thereby provides an opportunity to reveal if one's accommodative belief is wrong.

### Confirmation

Within a classic hypothetico-deductive system, an initial goal is to derive as many empirical implications of an explanation as possible. These implications are then to be tested by comparing these implications to actual observed data. Drawing on Carnap's [-@carnap1936, 425] \'gradually increasing confirmation\', this entails that each time a model matches the data, the confidence that the model is true is increased. If, on the other hand, the model fails, it can be discarded as untrue. This should thus lead to the continual rejection of false models, and move us ever closer to, but not necessarily to, the actual model of reality. Although certainly an enticing prospect, there are problems related to this approach, irrespective of any goals of establishing covering laws. 

A fundamental issue for hypothetico-deductivism, and scientific inference as a whole, follows from Hume's problem of induction [e.g. @ladyman2002, 31--61]. As an empiricist, all knowledge about the world was for Hume derived from sensory perception. Any reasoning that extend beyond observation, past or present, is based on cause and effect. However, since we can never observe a causal connection between events, the conjoined occurrence of observations is all we have to draw on. As there is no logical necessity for regularity in patterns to hold beyond what we can observe, there is no logical foundation for inductive reasoning -- there is no logical connection between the observable and unobservable. While we might observe the sun rise every day, there is no logical contradiction in believing it will not rise tomorrow. Hume held that while inductive reasoning will continue to be fundamental to science, and our every-day lives, it has no logical justification. Following from the problem of induction, an issue for hypothetico-deductivism therefore pertains to the value of testing an hypothesis, and whether with successful tests our belief in the hypothesis should increase. 

The logical empiricist attempts at working around the problem of induction and establishing a logical justification for confirmation was never successful, and a move to stating our beliefs in probabilistic terms never dissolved this fundamental issue. Central here is what is known as the paradoxes of confirmation [e.g. @sprenger2023], of which Hempel's own raven paradox is a classic example. If the hypothesis is that all ravens are black, this is logically equivalent to the statement that if something is not black it is not a raven. If we were to observe a black raven, this is evidence in support of the hypothesis. The paradox follows from the second statement: Given their logical equivalence, the observation of a green apple would be evidence in support of the hypothesis. Paradoxically then, we can study ravens by looking at apples. While problems of confirmation such as this are simple, they have proven difficult to resolve and a logically sound justification for confirmation is yet to be found [e.g. @godfrey-smith2003, 39--56].

### Falsification

One of the most influential contentions with the issue of testing is found with Popper and his concept of falsificationism. Popper, also a sceptic of induction, held that the problems of induction cannot be resolved. However, this is not of concern, as science in fact progresses not with confirmation but with falsification. In an attempt at demarcating science from non-science, Popper [e.g. -@popper1989, 33--66] stated that a theory can only be considered scientific if it has the potential to be refuted by observation. A theory that is compatible with all empirical variation is unscientific. The test of an hypothesis should be aimed at falsifying it, not confirming it, and an hypothesis that is not proven false should simply be subjected to even more stringent and elaborate tests. It is with each new rejection of an hypothesis that science progresses and we learn something about the world. Although it will inevitably be falsified, a good theory for Popper is therefore one that is bold, risky and corresponds with the world in surprising ways. There are, however, further issues related to the fundamental prospect of confronting our beliefs with data.  

As insight from complex systems theory demonstrates, sensitivity to initial conditions can lead both different causes to produce similar empirical results, and similar causes to produce different empirical results (van der Leeuw 2004:121; Premo 2010). This reflects the problems of equifinality and underdetermination, as presented above, where several models can agree on the empirical data, but disagree on the underlying causal mechanisms. This follows from the ubiquity of measuring error and the sensitivity of complex systems to minute variation. One classic example in this regard is the complex system of the weather, which can only be reliably predicted a few days into the future. Human systems are far more complex than that of the weather. Consequently, this renders the prospects of empirical confirmation or falsifiability weakened, and preference among different, even contradictory explanations, can often not hope to be based on observable data. In the case of archaeology, explanatory models are additionally faced with our generalisations of an already sparse and fragmented archaeological record, further increasing the likelihood that several explanations account equally well for the data at hand. However, this sensitivity to initial conditions can also impact the assumptions underlying an explanation. To show how this is an issue we can draw on what is known as the Duhem problem [after @duhem1914], which states that nothing is necessarily learned from rejecting an hypothesis on the grounds of a test.

Drawing on @hvidsten2014[184--187], we may postulate a simple model holding that mechanism A, under assumption B, implies C. In a test in which A operates, it would in a hypothetico-deductive understanding increase our belief in the model if we could reliably measure that C is true. In the case of Popper, the model is simply yet to be falsified. If, on the other hand, C is not true this would imply that either A or B are untrue. We would not, however, be able to derive logically which of A and B are untrue. This would perhaps not appear to be an immediate reason for concern. As long as one aspect of the model is untrue, the model is untrue, and should be rejected. The problem is that we know that models always contain a multitude of untrue assumptions. Drawing on the classic quote from Box above and the earlier discussion on abstraction, all models involve subsuming the virtual infinite complexity of reality and thus cannot work without an equal amount of untrue assumptions that could impact a test [a point made in the context of archaeology by @salmon1975]. 

In exemplifying the Duhem problem, @ladyman2002[77--78] gives the example of testing Newtonian gravitational theory by observing the travel of a comet. The theory of gravity alone does not provide a prediction for this path. It also depends on factors such as the mass of the comet, the mass of other objects in the solar system, and their relative positions, velocities and initial positions, as well as Newton's other laws of motion. If the test was to fail, this failure can follow from an untrue hypothesis, but also from a misspecification of an assumption that is subsumed in the test -- such as background conditions, measurement error, and initial conditions of the system. At some level a decision of whether the explanation has in fact been interfaced with observation is needed. As stated by @ladyman2002[80], \'falsification is only possible in science if there is intersubjective agreement among scientists about what is being tested.\' While a severely complicating issue for falsificationism, as Popper also recognised, his proposition still holds if it is qualified by stating that for a hypothesis to be scientific, it has to have the potential to be refuted by some kind of observation. It has to risk exposure to observation -- the challenge is determining what kind of observations this is [@godfrey-smith2003, 66].

Drawing on this issue with testing and falsificationism, several authors have argued that these factors alone do not determine the progression of science. A concern with understanding by what processes science has arrived at its current beliefs about the world is known as a naturalistic perspective. Philosophy of science should on this view not be concerned with establishing logically justified formalistic recipes for how research is to be conducted, but is itself best conceived of as an empirical study where scientific reasoning and argumentation is it's source material. While formalistic logic can provide important insights on what *can* constitute good components of strategies for scientific inquiry, such as aspects of Poppers falsificiationism, the scientific undertaking has been argued to be a far more messy enterprise. For example, the orbit of Mercury was not properly accounted for by Newtonian gravitational theory. It was not until decades later with Einstein's theory of gravity that this orbit was correctly predicted [@ladyman2002, 89]. Despite being unable to predict the orbit of Mercury, and thus being falsified, this did not cause the abandonment of Newton's theory of gravity. 

Examples illustrating this point can be found in archaeology as well. For example, when new dates that dramatically push back the earliest human occupation in the Americas have been presented over the years [e.g. @holen2017; @parenti1991; @parenti2018], these have often been met with scepticism as related to their veracity, and geological and other non-anthropogenic alternative explanations have been proposed [e.g. @braje2017; @magnani2019; @agnolin2023]. How convincing an explanation is and what causes it to be abandoned thus clearly depends on more than data alone, not least because data is more than a simple binary category that is either observed/not observed. What data is accepted, what it is understood to represent, and if it is adequately confronted with a hypothesis is in part dependent on a decision by the person who observes and the wider research community. What we observe should to some degree dictate what we believe about the world. However, the examples above demonstrate that stringent empiricism is untenable and is not in fact how scientific insight is achieved.

## Instrumentalism and scientific realism

Arguments such as those presented above have in sum both rendered suspect the empirical positivist’s absolute demand and adherence to observable data, and presents a significant challenge to the prospect of testing our beliefs about the world. Following from @levins1966, a defining element of modern MBA is consequently the realisation that models cannot at the same time maximise generality, realism and precision, which means models cannot and will never be constructed or evaluated purely on the grounds of observable data [@kohler2007, 7; @lake2015, 26]. However, if we were to concede to the fact that all models are wrong, how can we ever trust model-based inference?

In a classical instrumental understanding, the goal of science should be the prediction of phenomena that matter [e.g. @hausman1998, 187--190], a view famously forwarded by @friedman1953. Whether prediction is achieved through the use of models that build on true causal mechanisms or not is irrelevant. As long as the predictions of the model has a satisfactory correspondence with the empirical variation of interest, it is deemed a success. This view is therefore compatible with the constraining realisation that all models are wrong, both because the truth of postulated causal mechanisms in and of itself does not matter, and because of the resulting relaxed demand for accordance with total empirical variation -- degree of empirical correspondence determines the choice between models. 

Related views have also been advanced within archaeology. The most clear example can be found in the domain of archaeological locational (often also termed 'predictive') modelling, concerned with understanding where archaeological sites are located in the landscape [e.g. @verhagen2012]. These studies have sometimes focused on identifying where sites are located in the present-day landscape, irrespective of past motivations, so as to potentially reduce costs of land-development, or to help guide archaeological surveys in large areas where a complete coverage of the landscape is not possible. The concern then is knowing where sites are and are not located, not why. 

However, one of the criticisms forwarded towards instrumentalism is that if the ultimate goal is manipulation of relevant variables for the improvement of society, this will depend on uncovering true causal mechanisms. While mere prediction depends on stable correlation, control necessitates causality [@hausman1998, 190]. As @elster2015[18] puts it, explanation demands causation, and causation can never be revealed solely through prediction [see also @gibbon1989, 49]. One way to conceive of causality is as dependent on a counter-factual condition (e.g. @lewis1973; @morgan2015, 4--6), i.e. A causes C if when A occurs then C occurs, and if A does not occur then neither does C. Instrumentalism and a focus on prediction can therefore never hope to explain social phenomena [see also @lake2015, 23--24]. Of course, causal explanation does not necessarily have to be the main concern for archaeology. One could argue that academic interest in causal explanation should not always be the guiding principle behind archaeological inquiry but rather, for example, that mitigating costs associated with land-development or assembling interesting, albeit more speculative narratives about the past can be more important goals. My view in this context, as stated in the introduction to the thesis, follows from a form of realist understanding where scientific inquiry is as a strategy by which we try to confront theoretical constructs with empirical observation, aimed at aligning our beliefs as reliably as possible with what is true [@godfrey-smith2003, 161], where the ultimate aim is to answer why something we believe to be true has occurred.
	
Scientific realism entails the philosophical position that there exist real observable and unobservable entities and properties, and that claims concerning the veracity of either dimension cannot be set apart [@hausman1998; @psillos1999; @gibbon1989, 142--172; @wylie2002, 97--105]. The goal is to reveal these truths, where truth typically follows a commonsensical definition of being determined by what is the case, and not, for example, what we believe to be true or what is most beneficial [@ladyman2002, 157--158; see also @malnes2012, 19--30]. Regardless of whether or not it is possible to ever achieve, the goal of the realist is to reveal true, yet unobservable causal mechanisms that generate and shape the flux of observable phenomena. 

In a realist view, even the most careful empirical approach depends on theoretical assumptions that will determine what hypotheses are deemed relevant, what evidence empirical data are believed to represent, and how these are evaluated against hypotheses [@wylie2002, 100]. This point was also central to the early post-processual critique of processualism, where @hodder1984 argued that objective data is never tested against separate independent theories. These theories already underlie and determine how the archaeological material is recorded -- there is no theory-free data. To the realist, however, the realisation that we might view the world differently does not take away from the belief that we inhabit a common reality that exists and is true independently of what we think about it [@godfrey-smith2003, 174].  @shapin1985[355] stated that \'it is ourselves and not reality that is responsible for what we know.\' This, however, is a false dichotomy. As human knowledge is a part of reality, not something outside of it, it is better to understand human knowledge as the result of both ourselves and the world [@godfrey-smith2003, 132]. By extension, and by drawing on @fodor1984, @godfrey-smith2003[158--162] states that it is not enough to say that observation is theory-laden. The challenge is determining what theories influence observation, how they do so, and how reality manifests in observation. 

As an extension of this view, the form of feminist empiricist perspective advocated through Longino's [e.g. -@longino1990] contextual empiricism follows from treating the social group as the foundational scientific unit. What constitutes a good explanation in a field of research is determined by the varying views and non-coercive consensus that is reached on these issues at the level of the research community. As we view the world differently, what ideas are brought to bear on an issue, and a decision of whether a theory has been adequately interfaced with data will thus follow from the diversity of that community. This thus extends on Mill's \'marketplace of ideas\' as a scientific virtue. While there is a danger of simplistic generalisations of how for example sex differences influences how one views the world [@longino1990, ], a healthy state for a research community would thus be one where a multiplicity of marginalised and privileged groups are represented. For @longino1990, this does not entail a retreat to relativism, but rather a pragmatic view on objectivity.

Drawing on @longino2002, see thus objectivity as pragmatic and \'On this account, the goal of inquiry is not to produce knowledge claims that are true in all contexts and transcendent of local interest, but rather to warrant knowledge claims as credible given available resources, and reliable for specific purposes. Objectivity is, then, characterized in terms of norms of practice  something [see also @chapman2016, 11]

### Inference to the best explanation

So far induction has here been used to denote all non-deductive reasoning, and been exemplified by what is sometimes termed its enumerative or statistical form. That is, induction as the repeated observation of conjoined phenomena. However, other forms of non-deductive inference clearly exist. Archaeology is often, if not most often, concerned with explaining singular or infrequent events, and not generalisations where an appeal to enumerative induction is possible. Clearly then, other lines of reasoning can be drawn on to arrive at and choose between alternative explanations. One such form of inference has been variably labelled abduction, explanatory inference or inference to the best explanation [@godfrey-smith2003, 39--44; @harman1965; @lipton1991]. @lipton1991 formulates this mode of inference simply as \'Given our data and background beliefs, we infer what would, if true, provide the best of the competing explanations we can generate of those data.\' [@fogelin2007, 604]. Scientific realist often lean on this mode of inference to provide a way around the problems of induction and underdetermination, and this has been argued to constitute a good and often inadvertently employed framework for archaeological inquiry [e.g. @fogelin2007; @campanaro2021].

@fogelin2007 argues that despite the theoretical differences that exists among archaeologists, inference to the best explanation is often the logic underlying their conclusions. For example, he demonstrates how when providing an explanation for smudge pits, a common archaeological feature in Eastern United States, @binford1967 draws on ethnographic analogy to arrive at an explanation that is better than any alternative explanations he can muster [@fogelin2007, 611--612]. Despite using deductive-nomological language, Binford never independently tests any deductively derived hypothesis, and he arrives at his conclusion, @fogelin2007[612] argues, because it is the explanation among the alternatives that corresponds with the widest breadth of relevant empirical data. Similarly, @hodder1991, after having abandoned his most relativistic stance, adopts what he terms a \'guarded objectivity\' through an appeal to hermeneutics. This starts with the context of the archaeologists themselves and their pre-existing beliefs and underlying theories, which is opposed to the context of the people responsible for the archaeological material available to us. By moving back and forth between such context and trying to cast our data in the light of these, the goal is to adjust an interpretative whole until the two contexts coalesce. The process is thus one of iteratively fitting empirical pieces within an interpretative whole, that is at the same time adjusted by these pieces. In this framework \'We measure our success in this enmeshing of theory and data (our context and their context) in terms of how much of the data is accounted for by our hypothesis in comparison to other hypotheses.\' [@hodder1991, 8]. This is arguably also an appeal to inference to the best explanation [@fogelin2007, 612--614]. 

Central here is that hypotheses have been argued to be best evaluated when comparing them to the ability of substantive competing alternatives to fulfil the same purpose, and not just their negation, the null-model [e.g. @smith2015; @wylie2002, 95; @perreault2019]. Pitching alternatives against each other will lead away from a pure search for corroborative evidence for a single hypothesis, and, following from Chamberlin's [-@chamberlin1897] \'method of multiple working hypotheses\', help the researcher avoid \'a pressing of the theory to make it fit the facts and a pressing of the facts to make them fit the theory\' [@chamberlin1897, 843; see also @platt1964].

However, if one arrives at hypotheses that account for the data equally well, then other criteria will determine what is the best choice among them. A first criteria pertains to explanation, where a realist would hold that an hypothesis that makes claims about what has caused an empirical pattern will be given preference over an hypothesis that does not. If a locational model says that sites tend to be located close to rivers, and another explains this with reference to a specific kind of resource exploitation practice, then the second hypothesis would be given preference. This follows from the additional empirical implications this causal explanation can be assumed to hold -- in a Popperian view, it is more risky. Other virtues of a good explanation has been argued to be         

## What are models?

Building on the above, we can return to the issue of scientific models. While the classic hypothetico-deductive framework in a sense sees every model as a truth-candidate, they are for advocates of a model-based archaeology instead often understood as \'pieces of machinery that relate observations to theoretical ideas\' [@clarke1972, 1]. A similar view can be found with Morrison and Morgan's [-@morrison1999] view of \'models as mediators\', where a model is a concrete or explicit representation of observables or theoretical beliefs, and allows for a confrontation between these two dimensions. This is very much in line with the model as envisaged by @kohler2007, who sees them as constructions that have similarities with, but exist independently of the target systems that they are to represent. Models are constructions used to draw further inferences about the reality they are to represent, and are construed on the basis of what mechanisms we believe shaped the observables available to us. What is studied directly is the model, in the hope that when confronted with the world, the mechanisms of the model that the researcher is interested in correspond with those of the target system. This is how models have often been cast in a realist understanding, and variations on this are sometimes termed credible worlds, or idealised or isolating models (Gilbert 2008; Frigg & Hartmann 2018). These entail the inclusion of boundary conditions or assumptions considered essential for the model to function, the explicit or silent omission of aspects deemed unessential, and can involve an exaggeration of the characteristics of interest [@maki2009].

To explicate the concept of models as mediators, it can be useful to think in terms of an epistemological hierarchy, extending from observations to high-level theory [see @smith2011; @smith2015]. In a Mertonian view [@merton1968], this extends from day-to-day working hypothesis of what data represents, to middle-range theories that act as bridging concept for casting these as a more comprehensive high-level social theories [@raab1984; @smith2015, 22; see also @lucas2015 for nuances on this]. High-level theories can in this view be understood as \'overall perspectives from which one sees and interprets the world\' [@abend2008, 179], with examples frequently encountered in archaeology being practice theory, cultural evolutionary theory, and so on. Popper was concerned with establishing how Marxism and Freudian psychoanalysis were unscientific, as they are compatible will all empirical variation. However, it would seem misguided to try to falsify  [@godfrey-smith2003, 71]. Marxism cannot be falsified, but a given instantiation of Marxism, a Marxist model on the view taken here, should risk exposure to observation and have the potential to be falsified. One way to see models is thus as bridging concepts representing concrete instantiations of abstract theories, and as machinery for casting data as evidence to be confronted with these theoretical constructs.    

In a realist conception of models, these can thus be seen as analytical tools, the purpose of which is to provide a concrete representation of the researchers beliefs, used to isolate or create a closed and credible surrogate system where causal mechanisms are allowed to work without impediment from surrounding noise (see @sugden2000; @sugden2009; @cartwright2009; @mäki2009 for variations on this). The aim, according to @cartwright2009, is to reveal the capacities and differential contributions of unimpeded causal effects within such an idealised structure. However, this does not necessarily entail that the causal contribution is stable outside the surrogate system. In an open target system, the complex interplay of several causal mechanism can render the contribution from the modelled causal effects completely transformed, compared to their role in an idealised surrogate system [@gibbon1989, 150]. Therefore, although stable correlations can point to the possible existence of a causal relationship, the relevance of the realist study of capacities, unlike positivist regularities, does not presuppose closed target systems (Groff 2004:12–16). Positivism necessitates a closed system with regular conjunctions between events, such that an event of type A is always followed by an event of type B [@gibbon1989, 149]. @cartwright2009 contends that even though the realist surrogate system is credible, in the sense that the mechanisms could conceivably occur and result in the phenomena in question, the system is almost always different from all real cases in ways that matter. Drawing on the oft-invoked *ceteris paribus* statement -- all other things are in fact not equal (cf. Cartwright 2003[1983]:44–47) -- all models are wrong. The confrontation of model and data can therefore never avoid the problems of induction, and the question of interest then is not whether the model is true or false in its entirety, but if the model resembles the world in the relevant dimensions, given its purpose [@clarke2007, 747; @kohler2007, 3].

For all the ambiguities nested in the above account of what can be taken to constitute models, a central element is the view that they are constructed and explicit representations of our beliefs. Precisely this is also central to the contention that one of the most important aspect of model-based approaches follow from their explorative side [@hausman1992, 77; @aydinonat2007; @premo2010]. This results both from the assembly process itself, and from subsequent probing and manipulation of the model [@morrison1999]. In the initial construction of a representation of theory and data, the researcher is forced to concretise their assumptions and beliefs. This will likely lead to the adjustment of inconsistencies, the discovery of additional theoretical implications or relevant empirical patterns, and increase the opportunity for explicit handling and reporting of uncertainty. Through stringent and explicit aggregation of model features, further theoretical and empirical consequences are also likely to be revealed. Thus, in its construction, the model will already have provided valuable insights, regardless of its future archaeological life-span. Even so-called caricature models that are wildly unrealistic, extreme distortions have been argued to generate such insights (Gibbard & Varian 1978). 

Following its construction, further insight can be achieved through direct manipulation of model parameters and assumptions [@morrison1999, 32--35]. This holds the potential of revealing additional causal propensities and limitations that are difficult to reveal by passive study of the model, and can reveal how sensitive it is to such adjustments [@premo2010]. It has been argued that the potential of mathematical and computational models to stringently and coherently aggregate a multitude of mechanisms and allow these to interact over time, means that these can reveal unnoticed or counter-intuitive aggregate effects [@aydinonat2007], in effect generating new evidence [@chapman2016, 105--106]. The same exploratory potential is then extended by any attempts at evaluating the correspondence between model and target system, and by the involvement of an audience that comments, criticises, dismisses or helps align model and target system [@mäki2009].

## Archaeology, scaffolding and models

Theoretical discussions in archaeology have often framed the field as situated at extremes of positivism and relativism, or humanistic and scientific ideals, harking back to Snow's [-@snow1959] distinction between 'The Two Cultures' in western academia [e.g. @earle1987; @sørensen2017]. However, @chapman2016 and others [see @fogelin2007, above] have argued that this perspective does not inform how archaeology has in fact progressed, nor that it constitutes a good reference frame for understanding how to do good archaeology. This is not to say that these discussions cannot hold important points for elucidating the nature of our inferential frameworks, or that theoretical stances do not influence what questions are deemed of interest and how the material record is approached. Rather, this then naturalistic argument is that these discussions are over-simplified, hyperbole and largely unrepresentative of an archaeology that generally progresses by drawing on a far more complex web of theoretical and philosophical influences [see also @johnson2006; @pearce2011; @preston2014]. Furthermore, the extremes of insisting on trying to establish deductively certain knowledge or a whole-sale rejection of the possibility of ever moving beyond speculation does not represent an adequate reference frame for understanding what constitutes good archaeology, how to conduct it, nor how consensus and synthesis on claims about the past have been arrived at in the past.

Given the realisation that we lack an infallible logical foundation with which to establish explanations, @chapman2016 speak for an iterative epistemological process where a temporary scaffolding for how data is cast as evidence by drawing on multiple methodologies and lines of reasoning is continuously adjusted, extended and reassembled. Crucially, these scaffolds are to be subjected to critical reflexivity, but be grounded in domain-specific norms of what constitutes evidence, so as to tackle what @binford1981[21] presented as the challenge of \'how to keep our feet on the "empirical" ground and our heads in the "theoretical" sky.\' [@chapman2016, 8]. 

By drawing on @toulmin1958[213], who argues that we should \'abandon the ideal of analytic argument\' and the goal of deductive certainty, a central component of Chapman and Wylie's [-@chapman2016, 36--37] argument is illustrated by a quote from @toulmin1958[37]: \'The proper course for epistemology is neither to embrace nor to armour oneself against scepticism, but to moderate one's ambitions -- demanding of argument and claims of knowledge in any field not that they should measure up against analytic standards but, more realistically, that they shall achieve whatever sort of cogency or well-foundedness that can relevantly be asked for in that field.\' Important here is that there is no universal recipe for inferential adequacy, but that inference is domain specific. What we can hope to achieve is that our inferences are credible, but this limitation should not entail a regress into whole-sale scepticism. The goal is to arrive at beliefs that are more reasonable to trust than doubt, without demanding that this should be infallible and beyond critical scrutiny. 

As @clarke1973 Drawing on @norton, @chapman2016[40] terms this 'hypothetico-contextualism'.

Along similar lines, several authors have argued that archaeology is now characterised by methodological and theoretical eclecticism [@pearce2011]. As @preston2014[19] states, archaeology \'is intellectually *distinctive*, even if not intellectually *unified*. The latter, I would argue, is an inappropriate goal for the discipline\'. Considering the complexity of the questions we deal with, it is difficult to argue that this situation is anything but positive. Furthermore, accepting this eclectic state of affairs is appears a far more fruitful way of trying to understand archaeological practice, rather than insisting on casting this in some dichotomous light that poorly captures how the archaeological enterprise is actually undertaken. Furthermore, embracing eclecticism hopefully leads to a research-environment were practitioners are open to dialogue and are not overly committed to any single approach and dismissive of alternatives [@hemon2017, 233]. 

Building on this plurality, and echoing the point made by @godfrey-smith2003[] as referenced above, @chapman2016[41--43] argue that theory-ladenness will differentially impact what archaeologists consider evidence. Some biophysical observations will be relatively transferable between contexts, and their role as archaeological evidence less integrated with theoretical preconceptions. Inferences to do with symbolic behaviour is less transferable as they will be less secure and contingent on the given cultural context, and more dependent on the evidential scaffolding surrounding them to be considered evidentially adequate. However, this does not mean that symbolic behaviour is in any sense more off-limits than for example chronological inferences that draw on radiometric dating. Neither can reach deductive certainty, and their role as evidence for past events is differentially dependent on the warrants and assumptions that underlie them [@chapman2016, 42].  

Given the case where multiple models achieve empirical adequacy, that is, they predict the empirical pattern of concern equally well, realist have argued that inference to the best explanation is the best way forward and that this represents a way to handle the unavoidable problem of induction. 

While these might be sensible guiding principles for arriving at good explanations. @chapman2016[7] as aims at \'establishing claims that are empirically irreproachable they may foreclose (some) risk of error but at the expense of abandoning the very questions that make archaeology worth doing, and if they do not self-limit in this way they may have nothing to offer but speculation.\'. 

The understanding of archaeological inquiry outlined above need not be cast within a model-based understanding. The term model has been noted to increasingly involve aspects that were previously seen as a domain of theory [@preston], and their role has conceived of here also relates to other bridging concepts such as middle-range theory and evidential scaffolding. Furthermore, it has also been argued that models are best understood as a seperate


However, I believe the model terms offers a sensible way of thinking about these issues, as it forces us to see explanations as fallible explicit constructs, which are thus both more easily interrogated by others and less likely to lead to an explanatory complacency. Furthermore, models can, as with 



What are models?
“It is […] more appropriate to describe models than to attempt a hopelessly broad or hopelessly narrow definition for them” [@clarke1972, 2]

Following Clarke, no conclusive definition of models is sought here. The concept is notoriously difficult to pin down as its use varies across different disciplines, between various authors, and a multitude of classificatory schemas of model types have been proposed (e.g. Hausman 1992; Morrison & Morgan 1999a; Gilbert 2008:5; @godfrey-smith2009; @clarke2007). Nonetheless, some possible understandings of the term are presented below, mainly in order to set up an understanding of modelling in relation to the practice of archaeological inquiry and to understand what could demarcate model-based archaeology (MBA) from other forms of archaeological inference.


Godfrey-Smith's [-@godfrey-smith2009, 102] understanding of a model-based style of science sees this as a distinct approach, starting at remove from the phenomena, or target system, of interest. A model-based approach is in this understanding thus different from an analysis that starts by trying to describe and understand the system under study. Instead it begins with the exploration of a hypothetical, fictional model, simplified and largely independent of the target system, before this is ultimately interfaced with empirical data, involving thus 'a deliberate detour through fiction' [@godfrey-smith2009, 103]. As the modeller in this instance would be in control of all parts of the artificial model and their interactions, it is in the empirical confrontation that the model can yield the most fruitful results, either by capturing the empirical variation of interest or by allowing for an exploration and understanding of where it fails. 

 

A realist understanding holds that unobservable theoretical constructs should be conceived of as true mechanisms and events, and that these should not be distinguished from observables. In a realist view, the progression of science is not achieved by a random and undirected trial-and-error search by exploring empirical patterns, nor is it achieved by rejecting relevant hypotheses only on the basis of empirical adequacy. Our beliefs concerning unobservables shape how we observe, order and confront theoretical constructs with empirical data.   


Why model? 

The last chapter laid out the analytical framework that was used when thinking about these issue. The last section. This was not drawn on directly in the papers, but form a good frame both for considering their contribution and for setting up some future avenues along wich these could be explored.

<!-- One of the benefits of a MBA is thus that it aims at being a concrete representation of our beliefs about the world and allows this to be confronted with the world. When processual or New Archaeology entered the scene in the 1960s [e.g. @binford1962] this was based on criticism of how archaeology had been traditionally conducted, the main issue of which was argued to be a naive positivist and inductivist approach to the archaeological material [e.g. @gibbon1989, 61--90]. It was argued that in traditional archaeology the recovery and ordering of archaeological material was seen as an end in and of itself, following from a tacit belief that an understanding of the past would simply emerge when the material became systematised and adequately extensive. This is argued to have resulted in an inferential framework where culture was conceived of as a mentally internalised and aggregated unified concept, consisting of a univariate whole of norms or ideas. The archaeological record was to reflect this one model for cultural variation. Furthermore, by conceiving of culture as related to the mental domain, and cultural material as related to the empirical domain, this created an ontological break between the two dimensions, leading to a state where many cultural aspects relating to for example social relations and beliefs was not seen as manifest in the archaeological material. The empirical material will in this processual understanding therefore offer a direct link back to this systemic whole, thereby also collapsing the ontological break 
This view is normally ascribed to the critical realism of Bashkar (2008[1975]; Groff 2004; see Gibbon 1989 for an early account from an archaeological perspective).
-->
