# Model-based archaeology

Over the years, several works have purported the benefits of a model-based archaeology [@clarke2015, @wylie2002, 91--96], which has especially gained a foothold within the sub-field of computational archaeology (e.g. @kohler2007; @lake2015; @romanowska2019; @brughmans 2021). The goal of the next sections is twofold. First to elucidate what defines or can define a model-based scientific approach, and in the next chapter demonstrate how this can form a useful framework for archaeological inquiry by means of example. The following sections are loosely structured around four problem areas in the understanding scientific models, as identified by @frigg2018[1]: 1) The ontological: what are models? 2) The semantic: what do models represent? 3) The epistemological: how do we learn with models? And 4) what consequences do the use of models have for overarching principles such as scientific realism, reductionism and explanation?

## What are models?
“It is […] more appropriate to describe models than to attempt a hopelessly broad or hopelessly narrow definition for them” [@clarke2015, 2]

Following Clarke, no conclusive definition of models is sought here. The concept is notoriously difficult to pin down as its use varies across different disciplines, between various authors, and a multitude of classificatory schemas of model types have been proposed (e.g. Hausman 1992; Morrison & Morgan 1999a; Gilbert 2008:5; Godfrey-Smith 2009; @clarke2007). Nonetheless, some possible understandings of the term are presented below, mainly in order to set up an understanding of modelling in relation to the practice of archaeological inquiry and to understand what could demarcate model-based archaeology (MBA) from other forms of archaeological inference.

One fairly common understanding of models simply entail seeing them as a set of simplifications or assumptions concerning real-world phenomena [@barton2013, 154]. Any representation could thus be considered a model whether it is generated physically, digitally, verbally, simply imagined, or is construed in a natural or formal language. Scholars arguing the case for MBA often start out by making the point that whether we acknowledge it or not, we always employ such abstractions when attempting to understand past reality (Kohler & van der Leeuw 2007:4; Lake 2015:7). The infinite complexity of reality means that any description of it has to be a simplification, and even if we were able to, a complete rendition of reality would not be a worthwhile endeavour in its own right. A perfect reconstruction of reality would be a tautology, which without perspective offers neither insight nor understanding [@yarrow2006, 77]. Put differently, whether we understand archaeology as tasked with providing explanation, understanding, or interesting narratives about the past, any demand for a higher empirical resolution, for its own sake, would be a refutation of theory [see @healy2017]. These are, however, universal scientific points, variations of which have been made under diverse headings of archaeological theory (e.g. Johnson 2010:7; Robb & Pauketat 2013). It would thus follow that if the term model is taken to denote all generalisations or abstractions of reality, which in its ubiquity would include any description or explanation, it is not given why this would have to be dealt with within a comprehensive MBA. The arguments in favour of a distinct MBA tend to follow from *how* this necessary simplification should be made explicit, and in turn handled.

Godfrey-Smith's [@godfrey-smith2009, 102] understanding of a model-based style of science sees this as a distinct approach, starting at remove from the phenomena, or target system, of interest. A model-based approach is in this understanding thus different from an analysis that starts by trying to describe and understand the system under study. Instead it begins with the exploration of a hypothetical, fictional model, simplified and largely independent of the target system, before this is ultimately interfaced with empirical data, involving thus 'a deliberate detour through fiction' [@godfrey-smith2009, 103]. As the modeller in this instance would be in control of all parts of the artificial model and their interactions, it is in the empirical confrontation that the model can yield the most fruitful results, either by capturing the empirical variation of interest or by allowing for an exploration and understanding of where it fails. 

Another variation on the understanding of models also sees them as constructions that have similarities with, but exist independently of the target systems that they are to represent [@kohler2007]. Models are constructions used to draw further inferences about the reality they are to represent. How I understand this conception to be different from Godfrey-Smith's understanding is that here the models need not initially be conceived of independently of the target system. Models are here construed on the basis of what mechanisms we believe shaped the observables available to us. What is studied directly is the model, in the hope that the mechanisms of the model that the researcher is interested in correspond with those of the target system. These are sometimes termed idealised or isolating models (Gilbert 2008; Frigg & Hartmann 2018), and entail the exaggeration of the characteristics of interest, the inclusion of boundary conditions or assumptions considered essential for the model to function, and the explicit or silent omission of aspects deemed unessential [@maki2009].

One of the benefits of a MBA is thus that it aims at being a concrete representation of our beliefs about the world and allows this to be confronted with the world. When Processual or New Archaeology entered the scene in the 1960s [e.g. @binford1962] this was based on criticism of how archaeology had been traditionally conducted, the main issue of which was argued to be a naive positivist and inductivist approach to the archaeological material [e.g. @gibbon1989]. It was argued that traditionally the recovery and organisation of archaeological material was seen as an end in and of it itself, following from a tacit belief that an understanding of the past would simply emerge when the material became systematised and adequately extensive. This is argued to have resulted in an inferential framework where culture was conceived of as a mentally internalised and aggregated unified concept, consisting of a uni-variate whole of norms or ideas. The archaeological record was to reflect this one model for cultural variation. Furthermore, by conceiving of culture as related to the mental domain, and cultural material as related to the empirical domain, this created an ontological break between the two dimensions, leading to a state where many cultural aspects relating to beliefs and social relations was not seen as manifest in the archaeological material. If material culture is instead seen as an integrated part of and result of total cultural systems, explicit theories concerning how different aspects of cultural systems would influence and manifest in the material record could be presented.  On these grounds, New Archaeology therefore argued that the field should adopt the explanatory goals of the social sciences. This was to be achieved through the hypothetico-deductive approach and the covering-law framework for explanation, as taken from 

## Why model? 
'All models are wrong but some are useful' [@box1979, 2].

When constructing a model within a hypothetico-deductive (H-D) system, an initial goal is to derive as many empirical implications of an explanatory model as possible. These implications are then to be tested by comparing these implications to actual observed data. Each time a model matches the data, the confidence that the model is true is increased. If, on the other hand, the model fails, it can be discarded as untrue. This approach should thus lead to the continual rejection of false models, and move us ever closer to the actual model of reality (e.g. de Marchi 2005:6). Although certainly an enticing prospect, there are problems related to this approach. 

First, drawing on Hvidsten (2014:184–187), we may postulate a simple model containing assumptions A and B that together imply C. If, in the presence of A and B, we can reliably measure whether or not C is true, it would increase our belief in the model if C is true. If, on the other hand, C is not true this would imply that A and B are untrue. We would not, however, be able to derive logically whether both A and B are untrue or if it is only one of our assumptions that does not hold. In a H-D system this would not appear to be a concern. As long as one assumption is untrue, the model is untrue, and should be rejected. The problem is that we know that models always contain a multitude of untrue assumptions. Drawing on the classic quote from Box above and the earlier discussion on abstraction, all models involve subsuming the virtual infinite complexity of reality and thus cannot work without an equal amount of untrue assumptions. From this it follows that nothing is necessarily learned from rejecting a model on the grounds of untrue assumptions, and that models cannot be challenged purely and indiscriminately on the grounds of data, as this would, theoretically, lead to an infinite regress. 

Furthermore, several authors have argued that this is not how science in fact progresses. In the philosophy of science, this is known as a naturalistic perspective, which is concerned with understanding by precisely what processes science has gained knowledge about the world. While attempts at establishing formalistic recipies for undertaking research can offer important insights on what constitutes good strategies for scientific inquiry, such as the H-D system or Poppers more nuanced falsificationism, the scientific undertaking has been showed to in fact be a far more messy enterprise. For example, Galilei's work to establish the Copernican model of the solar system is widely held as a classic example of the success of science, and has often been used to illustrate the common understanding of the scientific method in which one moves from making an observation, establishing an hypothesis, testing it, and then rejecting or adjusting one's hypothesis before repeating the process. However, several historians of science have argued that in this framework the erroneous Aristotelian model should in fact have been preferred at the time, given the evidence at hand. On the grounds of data alone there would be no reason for Galileo to have given any credence to the Copernican model nor to persist with attempts at proving this alternative explanation. @gordfrey-smith2003 further exemplifies that if an experiment found that metal did not expand under, this would lead us to question the experimental design or raise concerns of measurement error -- it would not lead us to abandon the theory. Parallels can be found in archaeology. For example, when new dates that dramatically push back the earliest human occupation in the Americas have been presented over the recent years [e.g. @holen2017], these have often been met with scepticism as related to their veracity [@braje2017; @magnani2019]. How convincing an explanation is thus clearly depends on more than data, not least because data is more than a simple binary category that is either observed/not observed. What data is accepted and what it is understood to represent is dependent on the person who observes and the wider research community, with stronger demands likely to be directed at data that go against well-established theories. What we observe should to some degree dictate what we believe about the world. However, the examples above demonstrate that absolute, strong empiricism is untenable and not in fact how research is done.

Furthermore, as insight from complex systems theory demonstrates, sensitivity to initial conditions can lead both different causes to produce similar results, and similar causes to produce different results (van der Leeuw 2004:121; Premo 2010). This reflects the problems of equifinality and underdetermination, where several models can agree on the observable, but disagree on the unobservable causal mechanisms. This follows from the ubiquity of measuring error and the sensitivity of complex systems to minute variation. The classic example in this regard is the weather, which can only be reliably predicted a few days into the future. Human systems are more complex than that of the weather. Consequently, this renders the prospects of empirical verification or falsifiability further weakened, as preference among different, even contradictory models, cannot be based solely on observable data. In the case of archaeology, explanatory models are additionally faced with our generalisations of an already sparse and fragmented archaeological record, further increasing the likelihood that several explanatory models account equally well for the modelled data. 
	
Arguments such as those above have in sum rendered suspect the empirical positivist’s absolute demand and adherence to observable data. Following from @levins1966, a defining element of modern MBA is consequently the realisation that models cannot at the same time maximise generality, realism and precision, which means models cannot and will never be constructed or evaluated purely on the grounds of observable data (Kohler & van der Leeuw 2007:7; Lake 2015:26). However, if we were to concede to the fact that all models are wrong, how can we ever trust model-based inference?

In a classical instrumental understanding, the goal of science should be the prediction of phenomena that matter, a view most famously forwarded by Friedman (2008[1953]). Whether prediction is achieved through the use of models that are true or not is irrelevant. As long as the predictions of the model has a satisfactory correspondence with the empirical variation of interest, it is deemed a success. This view is therefore compatible with the constraining realisation that all models are wrong, both because the truth of the model in and of itself does not matter, and because of the resulting relaxed demand for accordance with total empirical variation -- degree of empirical adequacy determines the choice between models. Related views have also been advanced within archaeology. The most clear example can be found in the domain of archaeological locational (sometimes termed 'predictive') modelling, concerned with understanding where sites are located in the landscape. These studies has sometimes focused on identifying where sites are located in the present-day landscape, irrespective of past motivations, so as to potentially reduce costs of land-development, or to help guide archaeological surveys in large areas where a complete coverage of the landscape is not possible. However, one of the criticisms forwarded towards instrumentalism is that if the ultimate goal is manipulation of relevant variables for the improvement of society, this will depend on uncovering true causal mechanisms. While mere prediction depends on stable correlation, control necessitates causality (Hausman 1998:190). As Elster (2015:18) puts it, explanation demands causation, and causation can never be revealed solely through prediction (see also Gibbon 1989:49). Instrumentalism, therefore, can never hope to explain social phenomena. Of course, explanation does not necessarily have to be the main concern for archaeology. One could argue that academic interest in explanation should not be the guiding principle behind archaeological inquiry but rather, for example, that mitigating costs associated with land-development or assembling interesting narratives about the past are more important goals. My view, as stated in the introduction to the thesis, follows from a form of realist understanding of truth and that scientific inquiry is as a strategy by which we try to confront theoretical constructs with empirical observation, aimed at aligning our beliefs as reliably as possible with what is true [@godfrey-smith2003, 161].
	
Realism entails the philosophical position that there exist real observable and unobservable entities and properties, and that claims concerning either dimension cannot be set apart (@hausman1998; @psillos1999; @gibbon1989; @wylie2000, 97--105]. The goal is to reveal these truths, where truth typically follows a commonsensical definition of being determined by what is the case, necessitating a correspondence between the world and our description of it, and that truth is not, for example, what we believe to be true or what is most beneficial [@ladyman2002, 157--158; see also @malnes2012, 19--30]. Regardless of whether or not it is possible to ever achieve, the goal of the realist is to reveal true, yet unobservable causal mechanisms that generate and shape the flux of observable phenomena.  <!-- This view is normally ascribed to the critical realism of Bashkar (2008[1975]; Groff 2004; see Gibbon 1989 for an early account from an archaeological perspective).  --> In a realist view, even the most careful empirical approach depends on theoretical assumptions that will determine what hypotheses are deemed relevant, thus also determining how empirical data is treated and evaluated against these [@wylie2002, 100]. No meaningful separation can therefore be made between the veracity of observables and unobservables. This point was also central to the Post-Processual critique of New Arcaheology, where @hodder1984 pointed out that data is never tested against separate independent theories, as these theories already underlie and determine how the archaeological material is recorded and defined. However, to the realist this does not preclude testing altogether. An explanation has to adequately cover the perceived empirical variation

The H-D framework in a sense sees every model as a truth-candidate. Models can for the realist instead be understood as analytical tools, the purpose of which is to provide a concrete representation of the researchers beliefs, used to isolate or create a closed and credible surrogate system where causal mechanisms are allowed to work without impediment from surrounding noise (see Sugden 2000; Cartwright 2009; Mäki 2009 for variations on this; Sugden 2009). This is very much in line with the model as envisaged by Kohler and van der Leeuw (2007), as outlined above. The aim, according to @cartwright2009, is to reveal the capacities and differential contributions of unimpeded causal effects within an idealised structure. However, this does not necessarily entail that the causal contribution is stable outside the surrogate system. In an open target system, the complex interplay of several causal mechanism can render the contribution from the modelled causal effects completely transformed, compared to their role in an idealised surrogate system (Gibbon 1989:150). Therefore, although stable correlations can point to the possible existence of a causal relationship, the relevance of the study of capacities, unlike regularities, does not presuppose closed target systems (Groff 2004:12–16). Cartwright (2009) furthermore contends that even though the surrogate system is credible, in the sense that the mechanisms could conceivably occur and result in the phenomena in question, the system is almost always different from all real cases in ways that matter. Drawing on the oft-invoked *ceteris paribus* statement, all other things are typically not equal (cf. Cartwright 2003[1983]:44–47) -- all models are wrong. Cartwright (2009) proposes that a solution to this issue could be probing the model for sensitivity to changes in assumptions and omissions, but demonstrates that this is ultimately unsatisfactory. She exemplifies that if A and B imply C, A and D imply C, A and E imply C etc., can this mean that A and anything implies C? This relates to Hume's problem of induction, and the fact that even if we have observed. True causality can be understood as dependent on a counter-factual condition (e.g. Lewis 1973), i.e. if not A then not C. This echoes the earlier discussion on equifinality, and means that emulation does not appear to represent explanation, as this can never determine whether true causal mechanisms have been revealed (cf. @lake2015, 23--24). 

The confrontation of model and data can therefore never avoid the problem of induction. The question of interest then is not whether the model is true or false in its entirety, but if the model resembles the world in the relevant dimensions, given its purpose [@clarke2007, 747]. When evaluating a model in this view, the concern is if it corresponds with the world in ways that matter. A classic example in the literature is subway maps. These are clearly extreme simplifications of the world, and confronting them with how the world actually looks would easily show that they are not true renditions of reality. However, given it's purpose, the subway map should instead be evaluated by the degree to which it helps commuters get from A to B. Furthermore, any tests of models has been argued to be best done by comparing them to the ability of substantive competing alternatives to fulfill the same purpose, and not just their negation, the null-model [@smith2015; @wylie; @perrault]. Pitching alternative models against each other will lead away from a pure search for corroborative evidence, and. Relating this to the subway map, one could then for example compare its ability to help commuters to that of the topographic map. Neither map is a true representation of reality, but given the problem at hand, the subway map is likely to be the better alternative. While there might still exists other ways of representing the subway system that would be better than the current subway maps, given the current models at hand, the subway map would be the model of choice. 

Given the case where multiple models achieve empirical adequacy, that is, they predicts the empirical pattern of concern equally well, realist have argued that inference to the best explanation is the best way forward and that this represents a way to handle the otherwise unavoidable problem of induction. 

For all the ambiguities in the above account of what can be taken to constitute models, a common element is the view that they are constructed and explicit representations of our beliefs. Precisely this is also central to the contention that one of the most important aspect of model-based approaches follow from their explorative side (Hausman 1992:77; Aydinonat 2007; Premo 2010). This results both from the assembly process itself, and from subsequent probing and manipulation of the model (Morrison & Morgan 1999b). In the initial construction of a representation of theory and data, the researcher is forced to concretise their assumptions and beliefs. This will likely lead to the adjustment of inconsistencies, the discovery of additional theoretical implications or relevant empirical patterns, and increase the opportunity for explicit handling and reporting of uncertainty. Through stringent and explicit aggregation of model features, further theoretical and empirical consequences are also likely to be revealed. Thus, in its construction, the model will already have provided valuable insights, regardless of its future scientific life-span. Even so-called caricature models that are wildly unrealistic, extreme distortions have been argued to generate such insights (Gibbard & Varian 1978). Following its construction, it has been argued that further insight can be achieved through direct manipulation of model parameters (Morrison & Morgan 1999b:32-35). This holds the potential of revealing additional causal propensities and limitations that are difficult to reveal by passive study of the model, and can reveal how sensitive it is to such adjustments (Premo 2010). The same effects are subsequently extended by any attempts at evaluating the correspondence between model and target system, and by the involvement of an audience that comments, criticises, dismisses or helps align model and target system (Mäki 2009). 
A realist understanding holds that unobservable theoretical constructs should be conceived of as literally true mechanisms and events, and that these should not be distinguished from observables. In a realist view, the progression of science is not achieved by a random and undirected trial-and-error search by exploring empirical patterns, nor is it achieved by rejecting relevant hypotheses only on the basis of empirical adequacy. Our beliefs concerning unobservables shape how we observe, order and confront theoretical constructs with empirical data.   


# 
The last chapter laid out the analytical framework that was used when thinking about these issue. The last section. This was not drawn on directly in the papers, but form a good frame both for considering their contribution and for setting up some future avenues along wich these could be explored.

## Modelling the relationship between Mesolithic sites and the prehistoric shoreline

In the first paper of this thesis I have proposed a method for shoreline dating Mesolithic sites on the Norwegian Skagerrak coast, based on an empirically derived model of the relationship between the sites and the prehistoric shoreline [@roalkvam2023]. This was based on simulating the distance between sites and the shoreline using 67 ^14^C-dated sites and local reconstructions of shoreline displacement. The study found the sites to typically be located on or close to the shoreline up until some time just after 4000 BCE, when a few sites are located further from the shoreline. At around 2500 BCE there is a clear break, and the sites are from this point on situated further from and at variable distances from the shoreline. Building on these findings, the likely elevation of sites dating to earlier than 2500 BCE were, in aggregate, found to be reasonably approximated by the gamma function given in Figure. This is the model that forms the foundation of the proposed method for shoreline dating presented in the paper. 

In one sense this model is instrumental as the *reason* for the location of the sites has not been considered explicitly. By combining the present altitude of a site, its likely elevation above the shoreline when it was in use, and local shoreline displacement curves, this model makes it possible to assign a probabilistic absolute shoreline date to coastal sites in the region. On a realist view, however, it is still true that the treatment of the data and the conception of the model followed from an underlying belief of what mechanisms shaped the patterns in the data deemed relevant. While the model and derived method can be viewed as a instrumental dating tool, they are determined by the proclivity for sites to be located on the shoreline. As such, they are likely to be tightly integrated with both overarching cultural developments, as well as behaviour at the site level. By extension, the multitude of factors that can have shaped the site-sea relationship on the large and small scale, both temporally and spatially, offers a challenging causal web of possible interacting effects that ultimately determine this relationship. Having first derived this largely instrumental model, it gives opportunity to further test it's correspondence with other empirical data, and explore and expound underlying theoretical assumptions and implications. 

To illustrate this, below I have constructed a suggestion for a causal model concerning what determines the vertical distance between coastal Mesolithic sites and the shoreline in south-eastern Norway. The direction of the arrows in the model illustrates what variables are believed to impact other variables. An arrow going directly between two variables means that there is a direct effect. If there is a direct effect between A and B, but variable Z also impact each of these, Z is said to be confounder. An arrow from variable A to B that go through one or more other variables indicate that the effect of A on B is mediated by the intermediate variables. A central element of this model is that the effect of the other variables on the distance between site and shoreline are all mediatated through exposure and accessibility.

```{r dag}
knitr::include_graphics("daggity_model_png")
# dag {
# bb="0,0,1,1"
# "Distance from shoreline" [pos="0.557,0.314"]
# "Length of stay" [pos="0.248,0.312"]
# "Means of travel" [pos="0.187,0.496"]
# "Purpose of stay" [pos="0.191,0.105"]
# Accessibility [pos="0.421,0.402"]
# Exposure [pos="0.422,0.246"]
# Season [pos="0.077,0.312"]
# "Length of stay" -> Accessibility
# "Length of stay" -> Exposure
# "Means of travel" -> "Length of stay"
# "Means of travel" -> "Purpose of stay"
# "Means of travel" -> Accessibility
# "Means of travel" -> Exposure
# "Purpose of stay" -> "Length of stay"
# "Purpose of stay" -> Accessibility
# "Purpose of stay" -> Exposure
# Accessibility -> "Distance from shoreline"
# Exposure -> "Distance from shoreline"
# Exposure -> Accessibility
# Season -> "Length of stay"
# Season -> "Means of travel"
# Season -> "Purpose of stay"
# }
```


A likely important factor is the purpose of the visit to the site. Is the site meant to be used as a stop to rest and repair tools, to be used as a hunting camp or a location from where to acquire raw-materials for tool-production? Is it a base-camp for the entire group from where further forays are made, or is it meant to be a meeting place for several groups? The purpose of the stay is likely also to determine the length of the stay, which in turn might have implications for how close to the shoreline the site is established. A longer stay could for example mean that the site is more withdrawn from the shoreline, so as to make sure storm surges do not reach the site. 

Means of travel is also included in the model. Most travel is assumed to be done by boat in this period, which means accessibility to the site from the sea is likely to be of concern, as well the ability to safely beach and store the boats. However, some travel was also likely done by foot, for example from a base-camp to a site close by for gathering and processing resources such as shellfish, where the need for the carrying capacity offered by boats might not have been necessary. Travel by sledge on the ice is also a possible alternative. Not having to land boats could presumably have implications for the site-sea relationship. 

The season also presumably has implications for how often one had to establish camp, possibly reducing mobility in colder periods. It might also have implications for the kinds of dwelling structures that were necessary to erect, and likely determines the kinds of resources that were exploited, thus potentially impacting the purpose of the stay. The season is also believed to have implications for the degree of wind and wave-action at a location, thus affecting the exposure of the site to the elements, and impacting accessibility. Season is therefore given a direct effect on all of these variables.

Some variables and nuance that have been left out of the model are worth commenting on. The weather is for example likely to impact a lot of these factors, but is near if not entirely impossible to determine archaeologically. Furthermore, the purpose of the stay is here indicated using a single variable, but a stay need not, or perhaps likely did not have a single purpose. A simple example might be a case where multiple kinds of resources were to be exploited from a site. A possible alternative would be to operationalise these as individual variables, where for example the magnitude of seal-hunting and the gathering of hazelnuts to be done from the site is kept as separate variables. These would in turn likely be determined by factors such as the density of these resources in the landscape, their caloric return, their cost in terms of handling-time and -energy, and the potential prestige associated with hunting a specific species or the accruement of enough food to allow for sharing. Furthermore, the entire picture is also further complicated by other latent variables that are left of the model. Social structure, overarching mobility patterns, territoriality, group size and composition, as well as religious beliefs could all impact land-use, site-structure and ultimately how sites were positioned relative to the sea.  

However, I still believe the model forms a reasonable starting point and that it has the potential to reveal some important and true causal determinants for the site-sea relationship. A central challenge is of course how these factors are to be operationalised and determined archaeologically. The exercise of setting up the causal model is nonetheless useful in its own right, if not simply by forcing me to think through and concretise what elements I believe are important and how these are related. It also forms a framework that dictates how these would have to be handled statistically. Furthermore, with reference to the concept of inference to the best explanation presented in the last chapter


Some assumptions concerning the directionality of influence between the variables are made here, which might be discussed. For example, it is assumed here that the length of the stay influences how exposed the location is. This in a sense places primary weight on the planning of the inhabitants. One could envisage a situation where arrival in fair weather leads to a case where a worsening of the weather could prove that the location was in fact too exposed, and the site is moved. The purpose of the visit did in this case determine what was initially an acceptable degree of exposure, but, although not modelled here, the purpose of the visit might change with the weather.

Focusing first on its instrumental value, shoreline dating will often provide the highest resolution date that one can hope to achieve for a site, given that material to radiocarbon date is quite rare due to taphonomic loss, and as established typological frameworks in the region operate on the millennial scale. By facilitating a dating method, the model can thus be drawn on to explore traditional long-term chronological questions, such as the frequency of sites throughout the period [@roalkvam], the assessment of typological frameworks, or the timing and spread of various cultural phenomena, to give some examples. Furthermore, the finding that sites tend to be located further from the shoreline from around 4000 BCE and 2500 BCE, correspond with major socio-economic developments, where these dates roughly correspond to the first introduction and subsequent firm establishment of agriculture in the region. Although it still remains to be tested on data independent from where it was derived, the instrumental utility of the model is therefore clear.  

However, as it is difficult to determine the longevity of use and re-use of the open-air sites that dominate the Mesolithic record in the region, the number and duration of settlement events being dated in each instance is not clear. Previous consideration of these questions typically range from characterising the sites as the result of short visits of only a few hours up to a few months, and range from single visits to seasonal re-visits over a few decades---possibly centuries in the most extreme instances. However, given the resolution that shoreline dating provides, even at its best, the method does not by itself provide a precision high enough to weigh in on this issue, and can therefore not inform the number or length of stays within the determined date range. This has implications both for the questions that the method can be used to answer, and causal drivers behind site location that we can hope to disentangle using the model, given that these dimensions are likely to have been of importance for the location of the site relative to the sea.   [e.g. @bailey2007; @perrault2019]

Having established the model also opens up for a shifting of perspective back and forth from the large to the small scale, and from shoreline date to site location relative to the shoreline. Such an approach can illuminate implications of the model and its workings, and, in turn, potentially also feed back to and lead to a refinement of the model. One such example is now given by considering the location of the site Pauler 1, relative to sea according to the model.

The mean elevation of the site is masl. Based on the likely elevation of the site when it was in use, as informed by the shoreline model, the resulting shoreline date is. The benefit of having a model where all three parameters are clearly defined is that this allows us to shift perspective between the date of the use of a site and the implication this has for the relative position of the shoreline. Thus, looking now instead back from the calendar scale (the x-axis in Figure), to the likely elevation of the site above sea (the exponential function on the y-axis in Figure), and combining this with the elevation of the sea-level above it's present altitude at this time (represented by the displacement curve in Figure)--effectively rearranging the equation--thus allows for an instantiation of the model implications for individual sites in the spatial domain. In Figure, this is done by simulating the sea-level that the shoreline date implies.

Given the commutative nature of the relationship between shoreline date and the elevation of the site above sea-level, it is possible to translate directly between these dimensions and treat the resulting sea-level 

The model, while a reasonable approximation of the relationship between sites and shoreline in aggregate, could still be substantially off when applied to individual sites, as was demonstrated here by the in-depth analysis of Pauler 1. However, this model failure has to be qualified. First, the articulation and exploration of *how* the model is wrong has allowed for a further understanding of both the site and the model. Furthermore, this can also function in a step towards generating causal models explaining why, in any given case, the site was located as it was. While an immensely challenging task, this would   

The concept of shoreline dating touches upon such a wide range of issues that it can in sense be seen as a microcosm of archaeological inquiry as such. While physical sciences underlie the framework, as it is ultimately dependent on reconstruction of shoreline displacement, the question is inherently social and cultural. Furthermore, perspectives from a vantage of social and humanistic can not only be used to derive cultural significance from the observed patterns, but these can also be used to further improve the method for shoreline dating. My view is that this is defining of archaeology as a whole. While here nested in model-based archaeology, which I found a useful framework with which to think about these issues, I also think this illustrates the heterogeneous nature of archaeology and the value of drawing on multiple strains of evidence and perspectives. The iterative move between aggreagte model and individual cases could just as easily have been cast within a hermeneutic understanding of archaeological research. This also highlights the inadequacy of attempting to understand archaeological research as being situated somewhere on a scale between more or less scientific or humanistic, more or less processual or post-processual, as these are simply unable to capture the necessary nuance of archaeological inquiry.   
