Model-based archaeology and the relationship between sites and sea in the Norwegian Mesolithic

Work purporting the benefits of a model-based archaeology has seen a dramatic increase over the last years, especially within the sub-field that can be tentatively labelled 'computational archaeology' (e.g. @kohler2007; @lake2015; @romanowska2019; @brughmans 2021). The goal of this text is twofold, first to elucidate what defines or can define a model-based scientific approach, and in turn demonstrate how this can form a useful framework for archaeological inquiry by means of example. The first part of the text is loosely structured around four problem areas in the understanding scientific models, as identified by @frigg2018[1]: 1) The ontological: what are models? 2) The semantic: what do models represent? 3) The epistemological: how do we learn with models? And 4) what consequences do the use of models have for overarching principles such as scientific realism, reductionism and explanation? Having offered some perspectives on these four problem areas, the second part of the text moves on to consider a model of the relationship between Mesolithic sites and the prehistoric shoreline along the Norwegian Skagerrak coast. This is aimed at giving a concrete example of how a model-based approach can be useful.

# What are models?
“It is […] more appropriate to describe models than to attempt a hopelessly broad or hopelessly narrow definition for them” [@clarke2015, 2]

Following Clarke, no conclusive definition of models is sought here. The concept is notoriously difficult to pin down as its use varies across different disciplines, between various authors, and a multitude of classificatory schemas of model types have been proposed (e.g. Hausman 1992; Morrison & Morgan 1999a; Gilbert 2008:5; Godfrey-Smith 2009). Nonetheless, some possible understandings of the term are presented below, but mainly in order to set up an understand of modelling in relation to the practice of archaeological inquiry and to understand what could demarcate model-based archaeology (MBA) from other forms of archaeological inference.

One fairly common understanding of models simply entail seeing them as a set of simplifications or assumptions concerning real-world phenomena [@barton2013, 154]. Any representation could thus be considered a model whether it is generated physically, digitally, verbally, simply imagined, or is construed in a natural or formal language. Scholars arguing the case for MBA often start out by making the point that whether we acknowledge it or not, we always employ such abstractions when attempting to explain past reality (Kohler & van der Leeuw 2007:4; Lake 2015:7). The infinite complexity of reality means that any description of it has to be a simplification, and even if we were able to, a complete rendition of reality would not be a worthwhile endeavour in its own right. A perfect reconstruction of reality would be a tautology, which without perspective offers neither insight nor understanding [@yarrow2006, 77]. Put differently, whether we understand archaeology as tasked with providing explanation, understanding, or interesting narratives about the past, any demand for a higher empirical resolution, for its own sake, would be a refutation of theory [see @healy2017]. These are, however, universal scientific points, variations of which have been made under diverse headings of archaeological theory (e.g. Johnson 2010:7; Robb & Pauketat 2013). It would thus follow that if the term model is taken to denote all generalisations or abstractions of reality, which in its ubiquity would include any description or explanation, it is not given why this would have to be dealt with within a comprehensive MBA. The arguments in favour of a distinct MBA that lean on this understanding of models tend to follow from *how* this necessary simplification should be made explicit, and in turn handled.

Godfrey-Smith's [@godfrey-smith2009, 102] understanding of a model-based style of science instead sees this as a distinct approach, starting at remove from the phenomena, or target system, of interest. A model-based approach is in this understanding thus different from an analysis that starts by trying to describe and understand the system under study. Instead it begins with the exploration of a hypothetical, fictional model, simplified and largely independent of the target system, before this is ultimately interfaced with empirical data, involving thus 'a deliberate detour through fiction' [@godfrey-smith2009, 103]. As the modeller in this instance would be in control of all parts of the artificial model and their interaction, it is in the empirical confrontation that the model can yield the most fruitful results, either by capturing the empirical variation of interest or by allowing for an exploration and understanding of where it fails. 

Another variation on the understanding of models sees them as constructions that have similarities with, but exist independently of the target systems that they are to represent [@kohler2007]. This means that models are constructions used to draw further inferences about the reality they are to represent. What is studied directly is the model, in the hope that the features of the model that the researcher is interested in correspond with those of the target system. These are sometimes termed idealised or isolating models (Gilbert 2008; Frigg & Hartmann 2018), and entail the exaggeration of the characteristics of interest, the inclusion of boundary conditions or assumptions considered essential for the model to function, and the explicit or silent omission of aspects deemed unessential [@maki2009].

# Why model? 
'All models are wrong but some are useful' [@box1979, 2].

When constructing a model within a hypothetico-deductive (H-D) system of empirical positivism (see for example @gibbon1989 for a critical, but thorough account), an initial goal is to derive as many empirical implications of an explanatory model as possible. These implications are then to be tested by comparing the implications to actual observed data. Each time a model matches the data, the confidence that the model is true is increased. If, on the other hand, the model fails, it can be discarded as untrue. This approach should thus lead to the continual rejection of false models, and move us ever closer to the actual model of reality (e.g. de Marchi 2005:6). Although certainly an enticing prospect, there are problems related to this approach. 

First, drawing on Hvidsten (2014:184–187), we may postulate a simple model containing assumptions A and B that together imply C. If we can reliably measure whether or not C is true, it would increase our belief in the model if C is true. If, on the other hand, C is not true this would imply that A and B are untrue. We would not, however, be able to derive logically whether both A and B are untrue or if it is only one of our assumptions that does not hold. In a H-D system this would not appear to be a concern. As long as one assumption is untrue, the model is untrue, and should be rejected. The problem is that we know that models always contain a multitude of untrue assumptions. Drawing on the classic quote from Box above and the earlier discussion on abstraction, all models involve subsuming the virtual infinite complexity of reality and thus cannot work without an equal amount of untrue assumptions. From this it follows that nothing is necessarily learned from rejecting a model on the grounds of untrue assumptions, and that models cannot be challenged purely and indiscriminately on the grounds of data, as this would, theoretically, lead to an infinite regress. 
	
Additionally, as insight from complex systems theory demonstrates, sensitivity to initial conditions can lead both different causes to produce similar results, and similar causes to produce different results (van der Leeuw 2004:121; Premo 2010). This reflects the problem of equifinality or underdetermination, where several models can agree on the observable, but disagree on the unobservable. This follows from the ubiquity of measuring error and the sensitivity of complex systems to minute variation.  Consequently, this renders the prospects of empirical verification or falsifiability further weakened, as such cases would not allow for any preference among different, even contradictory models, based solely on observable data. In the case of archaeology, explanatory models are additionally faced with our generalisations of an already sparse and fragmented archaeological record, further increasing the likelihood that several explanatory models account equally well for the modelled data. 
	
Arguments such as those above have in sum rendered suspect the empirical positivist’s absolute demand and adherence to observable data. Following from Levins (1966), a defining element of modern MBA is consequently the realisation that models cannot at the same time maximise generality, realism and precision, which means models cannot be constructed or evaluated purely on the grounds of observable data (Kohler & van der Leeuw 2007:7; Lake 2015:26). However, if we were to concede to the fact that all models are wrong, how can we ever trust model-based inference?

In a classical instrumental understanding, the goal of science should be the prediction of phenomena that matter, a view most famously forwarded by Friedman (2008[1953]). Whether prediction is achieved through the use of models that are true or not is irrelevant. As long as the predictions of the model has a satisfactory correspondence with the empirical variation of interest, it is deemed a success. This view is therefore compatible with the constraining realisation that all models are wrong, both because the truth of the model in and of itself does not matter, and because of the relaxed demand for accordance with total empirical variation. Related views have also been advanced within archaeology. The most clear example can be found in the domain of archaeological locational (sometimes termed 'predictive') modelling, which has traditionally been divided into a 'deductive' and 'inductive' approach. The concern of the latter is identifying where sites are located in the present day landscape, irrespective of past motivations, so as to potentially reduce costs of land development, or to help guide archaeological surveys in large areas where a complete coverage of the landscape is not possible.  However, one of the criticisms forwarded towards instrumentalism is that if the ultimate goal is manipulation of relevant variables for the improvement of society, this will depend on uncovering true causal mechanisms. While mere prediction depends on stable correlation, control necessitates causality (Hausman 1998:190). As Elster (2015:18) puts it, explanation demands causation, and causation can never be revealed solely through prediction (see also Gibbon 1989:49). Instrumentalism, therefore, can never hope to explain social phenomena. Of course, explanation does not necessarily have to be the main concern for archaeology. One could argue that academic interest in explanation should not be the guiding principle behind archaeological inquiry but rather, for example, that mitigating costs associated with land development or assembling interesting narratives about the past are more important goals. My view, however, follows from a form of realist understanding of truth and that scientific inquiry is as a form of strategy by which we try to confront theoretical constructs with empirical observation, aimed at aligning our beliefs as reliably as possible with what is true [@godfrey-smith2003, 161].
	
Realism entails the philosophical position that there exist real observable and unobservable entities and properties (Hausman 1998; Psillos 1999) 1. The goal is to reveal these truths, where truth typically follows the commonsensical definition of being determined by what is the case, and not, for example, what we believe or what it most beneficial. Regardless of whether or not it is possible to ever achieve, the goal of the realist is to reveal true, yet unobservable causal mechanisms that generate and shape the flux of observable phenomena. This view is normally ascribed to the critical realism of Bashkar (2008[1975]; Groff 2004; see Gibbon 1989 for an early account from an archaeological perspective). Where the H-D framework in a sense sees every model as a truth-candidate, models are for the realist instead analytical tools, the purpose of which is to isolate or create a closed and credible surrogate system where causal mechanisms are allowed to work without impediment from surrounding noise (see Sugden 2000; Cartwright 2009; Mäki 2009 for variations on this; Sugden 2009). This is very much in line with the model as envisaged by Kohler and van der Leeuw (2007), as outlined above. The aim, according to Cartwright (2009), is to reveal the capacities and differential contributions of unimpeded causal effects within an idealised structure. However, this does not necessarily entail that the causal contribution is stable outside the surrogate system. In an open target system, the complex interplay of several causal mechanism can render the contribution from the modelled causal effects completely transformed, compared to their role in an idealised surrogate system (Gibbon 1989:150). Therefore, although stable correlations can point to the possible existence of a causal relationship, the relevance of the study of capacities, unlike regularities, does not presuppose closed target systems (Groff 2004:12–16). Cartwright (2009) furthermore contends that even though the surrogate system is credible, in the sense that the mechanisms could conceivably occur and result in the phenomena in question, the system is almost always different from all real cases in ways that matter. Drawing on the oft-invoked *ceteris paribus* statement, all other things are typically not equal (cf. Cartwright 2003[1983]:44–47)---all models are wrong. Cartwright (2009) proposes that a solution to this issue could be probing the model for sensitivity to changes in assumptions and omissions, but finds this ultimately unsatisfactory. She exemplifies that if A and B imply C, A and D imply C, A and E imply C etc., can this mean that A and anything implies C? A problem with this relates to the problem of induction and how causality can be understood as dependent on a counter-factual condition (e.g. Lewis 1973), i.e. if not A then not C. This echoes the earlier discussion on underdetermination, and means that emulation is not explanation, as this cannot reveal causal mechanisms (cf. Lake 2015:23–24). This view is also what underlies the further specified exploratory view of models.

For all the ambiguities in the above account of what can be taken to constitute models, a common element is the view that they are constructed and explicit representations. Precisely this is also central to the contention that the most important aspect of model-based approaches follow from their explorative side (Hausman 1992:77; Aydinonat 2007; Premo 2010). This results both from the assembly process itself, and subsequent probing and manipulation of the model (Morrison & Morgan 1999b). In the initial construction of a representation of theory or data, the researcher is forced to concretise their assumptions and beliefs. This will likely lead to the adjustment of inconsistencies, the discovery of additional theoretical implications or empirical patterns, and increase the opportunity for explicit handling and reporting of uncertainty. Through stringent and explicit aggregation of model features, further theoretical and empirical consequences are also likely to be revealed. Thus, in its construction, the model will already have provided valuable insights, regardless of its future scientific lifespan. Even so-called caricature models that are wildly unrealistic, extreme distortions have been argued to generate such insights (Gibbard & Varian 1978). The same effects are subsequently extended by any attempts at evaluating the correspondence between model and target system, and by the involvement of an audience that comments, criticises, dismisses or helps align model and target system (Mäki 2009). However, the most rewarding exploration of a model, following its construction, has been argued to be achieved through direct manipulation of model parameters (Morrison & Morgan 1999b:32-35). In a realist view, this holds the potential of revealing additional causal propensities and limitations that are difficult to reveal by passive study of the model (Premo 2010). 

# Modelling the relationship between Mesolithic sites and the prehistoric shoreline

In the second paper of this thesis I have proposed a method for shoreline dating Mesolithic sites on the Norwegian Skagerrak coast, based on a empirically derived model of the relationship between the sites and the prehistoric shoreline [@roalkvam2023]. This was based on simulating the distance between sites and the shoreline using 67 ^14^C-dated sites and local reconstructions of shoreline displacement. The study found the sites to typically be located on or close to the shoreline up until some time just after 4000 BCE, when a few sites are located further from the shoreline (Figure). At around 2500 BCE there is a clear break, and the sites are from this point on situated further from and at variable distances from the shoreline. Building on these findings, the likely elevation of sites dating to earlier than 2500 BCE were, in aggregate, found to be reasonably approximated by the gamma function given in Figure. This is the model that forms the foundation of the proposed method for shoreline dating. This model is instrumental in the sense that the reason for the location of the sites has not been considered. However, by combining the present altitude of a site, its likely elevation above the shoreline when it was in use, and local shoreline displacement curves, this model makes it possible to assign a probabilistic absolute shoreline date to coastal sites in the region (@roalkvam 2023, an example is given in Figure, below). While the model and derived method can be viewed as a purely instrumental dating tool, they are determined by the proclivity for sites to be located on the shoreline. As such, they are likely to be tightly integrated with both overarching cultural developments as well as behaviour at the site level. By extension, the multitude of factors that can have shaped the site-sea relationship on the large and small scale, both temporally and spatially, offers a challenging causal web of possible interacting effects that ultimately determine this relationship. Having first derived the instrumental model, however, does give opportunity to explore such factors by examining the implications of the model along these dimensions.   

Focusing first on its instrumental value, shoreline dating will often provide the highest resolution date that one can hope to achieve for a site, given that material to radiocarbon date is quite rare due to taphonomic loss, and as established typological frameworks in the region operate on the millennial scale. By facilitating a dating method, the model can thus be drawn on to explore traditional long-term chronological questions, such as the frequency of sites throughout the period [@roalkvam], the assessment of typological frameworks, or the timing and spread of various cultural phenomena, to give some examples. Furthermore, the finding that sites tend to be located further from the shoreline from around 4000 BCE and 2500 BCE, correspond with major socio-economic developments, where these dates roughly correspond to the first introduction and subsequent firm establishment of agriculture in the region. Although it still remains to be tested on data independent from where it was derived, the instrumental utility of the model is therefore clear.  

However, as it is difficult to determine the longevity of use and re-use of the open-air sites that dominate the Mesolithic record in the region, the number and duration of settlement events being dated in each instance is not clear. Previous consideration of these questions typically range from characterising the sites as the result of short visits of only a few hours up to a few months, and range from single visits to seasonal re-visits over a few decades---possibly centuries in the most extreme instances. However, given the resolution that shoreline dating provides, even at its best, the method does not by itself provide a precision high enough to weigh in on this issue, and can therefore not inform the number or length of stays within the determined date range. This has implications both for the questions that the method can be used to answer, and causal drivers behind site location that we can hope to disentangle using the model, given that these dimensions are likely to have been of importance for the location of the site relative to the sea.   [e.g. @bailey2007; @perrault2019]

Having established the model also opens up for a shifting of perspective back and forth from the large to the small scale, and from shoreline date to site location relative to the shoreline. Such an approach can illuminate implications of the model and its workings, and, in turn, potentially also feed back to and lead to a refinement of the model. One such example is now given by considering the location of the site Pauler 1, relative to sea according to the model.

The mean elevation of the site is masl. Based on the likely elevation of the site when it was in use, as informed by the shoreline model, the resulting shoreline date is. The benefit of having a model where all three parameters are clearly defined is that this allows us to shift perspective between the date of the use of a site and the implication this has for the relative position of the shoreline. Thus, looking now instead back from the calendar scale (the x-axis in Figure), to the likely elevation of the site above sea (the exponential function on the y-axis in Figure), and combining this with the elevation of the sea-level above it's present altitude at this time (represented by the displacement curve in Figure)--effectively rearranging the equation--thus allows for an instantiation of the model implications for individual sites in the spatial domain. In Figure, this is done by simulating the sea-level that the shoreline date implies.

Given the commutative nature of the relationship between shoreline date and the elevation of the site above sea-level, it is possible to translate directly between these dimensions and treat the resulting sea-level 

The model, while a reasonable approximation of the relationship between sites and shoreline in aggregate, could still be substantially off when applied to individual sites, as was demonstrated here by the in-depth analysis of Pauler 1. However, this model failure has to be qualified. First, the articulation and exploration of *how* the model is wrong has allowed for a further understanding of both the site and the model. Furthermore, this can also function in a step towards generating causal models explaining why, in any given case, the site was located as it was. While an immensely challenging task, this would   

The concept of shoreline dating touches upon such wide range of issues that it can in sense be seen as a microcosm of archaeological inquiry as such. While physical sciences underlie the framework as it is ultimately dependent on reconstruction of shoreline displacement, the question is inherently social and cultural. Furthermore, perspectives from a vantage of social or humanistic can not only be used to derive cultural significance from the observed patterns, but these can also be used to further improve the method for shoreline dating. My view is that this is defining of archaeology as a whole. While here nested in model-based archaeology, which I found a useful framework with which to think about these issues, I also think this illustrates the heterogeneous nature of archaeology and the value of drawing on multiple strains of evidence and perspectives. The iterative move between aggreagte model and individual cases could just as easily have been cast within a hermeneutic understanding of archaeological research. This also highlights the inadequacy of attempting to understand archaeological research as being situated somewhere on a scale between more or less scientific or humanistic, more or less processual or post-processual, as these are simply unable to capture the necessary nuance of archaeological inquiry and potentially stand to limit   
