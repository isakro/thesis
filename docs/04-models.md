# Model-based archaeology

Over the years, several works have purported the benefits of a model-based archaeology [@clarke1972, @wylie2002, 91--96], which has especially gained a footing within the sub-field of computational archaeology (e.g. @kohler2007; @lake2015; @romanowska2015; @brughmans2021). The goal of the next sections is twofold. First to elucidate what defines or can define a model-based scientific approach, and in the next chapter to demonstrate how this can form a useful framework for archaeological inquiry by drawing on examples from the papers of this thesis. Central to the the following sections are four problem areas in the understanding scientific models, as identified by @frigg2018[1]: 1) The ontological: what are models? 2) The semantic: what do models represent? 3) The epistemological: how do we learn with models? And 4) what consequences do the use of models have for overarching principles such as scientific realism, reductionism and explanation?

One fairly common understanding of models simply entail seeing them as a set of simplifications or assumptions concerning real-world phenomena [@barton2013, 154]. Any representation could thus be considered a model whether it is generated physically, digitally, verbally, simply imagined, or is construed in a natural or formal language. Scholars arguing the case for MBA often start out by making the point that whether we acknowledge it or not, we always employ such abstractions when attempting to understand past reality [@kohler2007, 4; @lake2015, 7). The infinite complexity of reality means that any description of it has to be a simplification, and even if we were able to, a complete rendition of reality would not be a worthwhile endeavour in its own right. A perfect reconstruction of reality would be a tautology, which without perspective offers neither insight nor understanding [@yarrow2006, 77]. Put differently, whether we understand archaeology as tasked with providing explanation, understanding, or interesting narratives about the past, any demand for a higher empirical resolution, for its own sake, would be a refutation of theory [see @healy2017]. These are, however, universal scientific points, variations of which have been made under diverse headings of archaeological theory (e.g. Johnson 2010:7; Robb & Pauketat 2013). It would thus follow that if the term model is taken to denote all generalisations or abstractions of reality, which in its ubiquity would include any description or explanation, it is not given why this would have to be dealt with within a comprehensive MBA. The arguments in favour of a distinct MBA tend to follow from *how* this necessary simplification should be made, and in turn handled. What this entails can be foreshadowed here by invoking the classic quote from @box1979[2]: 'All models are wrong but some are useful'. But if all models are wrong, what is their epistemic value? To begin to answer this question, the above view of models, simply seeing them as abstractions, will be accepted for now without regard for their demarcation to data, theory and hypotheses.

## Confronting archaeoloigcal beliefs with archaeological data

The explicit testing of archaeological explanations was introduced to the discipline with processualism, which argued that archaeology should adopt the explanatory goals of positivist social sciences, or at least positivist social science as it was understood by processual practitioners at the time. How this was to be done first follows from the standard processual view on what the archaeological material represents. Here, material culture was seen as an integrated part of -- and the result of -- total, multidimensional cultural systems [e.g. @binford1962]. As such, theories concerning how all aspects of cultural systems would influence and manifest in the material record should be conceived. Central to this is that the archaeological material represents an objective, albeit complex empirical record that reflects empirical causes, irrespective of our beliefs about what these causes are. The empirical material will in this processual understanding therefore offer a direct link back to this systemic whole. Archaeological material is therefore representative of the multidimensional causal chain from cultural system to the archaeological record. Following from this, theories concerning the prehistoric systemic whole and what processes have influenced the remnants available to us must therefore come prior to an archaeological investigation for it to be possible to evaluate their veracity. As complex integrated wholes, such models of entire systems were then to be tested by drawing on the hypothetico-deductive approach. Furthermore, drawing on the covering-law framework as taken from logical positivism/empiricism, the ultimate goal was to establish laws pertaining to the conjunct occurrence of certain types of material remains with certain types of societal systems, irrespective of time and place.

 theoretical scientific terms are compatible with empiricist understanding of meaning as , and that observations can confirm theoretical concepts. The goal was to establish laws that are deductively valid, of the kind given by the classic example *All men are mortal \/ Socrates is a man \/ Therefore, Socrates is mortal*. If the premises are true, then the conclusion will always be true. However, whether an argument is deductively valid or not is not dependent on whether or not the premises are true. If it happens to be true that all men are mortal and Socrates is a man, then the deductively valid argument is said to be sound. However, determining whether the premises are true depends on non-deductive reasoning.     

It should be noted here that the programme of logical positivism, and the more mature logical empiricism, were far more nuanced that what they are typically given credit for in the archaeological literature concerned with establishing why these views were misguided [see @gibbon1989]. This is equally true for the over-simplifying presentation that is given here. However, this can in part be justified with reference to the naive versions of these programmes that were adopted by positivist social science and archaeology at the time. When adapted to archaeology, the search for general laws 
The search for covering laws was therefore quickly abandoned, with @flannery1973[] stating that the attempt at adopting Hempelian empiricism \'has produced some of the worst archaeology on record\'. However, abandoning the search for covering laws need not entail that all aspects of hypothetico-deductivsim is misguided, and more modest goals of testing hypotheses pertaining to specific contexts or research questions could, and is, I will argue, still viable.

@smith2015 [@smith2015; @smith2017] has stated that one of the most central questions we can ask about our archaeological explanations is 'How would you know if you are wrong?'. Archaeological explanation often take the form of what @binford1981 termed a *post hoc* accommodative argument. This involves first gathering and categorising the data of interest, often using variables chosen by convention and convenience, and then building an explanation around any discerned patterns [@clark2009, 29]. This inductive data-dredging or pattern-searching approach is argued to constitute a limited inferential framework for a couple of reasons. First, what among the virtually infinite aspects of the material available to us is considered interesting will always be determined by our beliefs concerning the processes that have resulted in their manifestation. What characteristics of the material is recorded and drawn on to organise it will dictate what patterns one can hope to reveal. Without an underlying theory, how would we, as @popper put it, know what to look for? If one follows what has been done conventionally, without taking any explicit stance towards this, one will be dependent on how others have conceived of this. Furthermore,  *Post hoc* accommodative arguments can provide the identification of empirical patterns with respects to the employed units of analysis, which in turn can form the basis for social and behavioural hypotheses. However, @binford1981[85] has argued that such arguments can at best be \'treated as provocative ideas in need of evaluation\'. @clark2009[29] argues that a necessary next step is to derive empirical implications of this hypothesis, which can be evaluated against a part of the archaeological record that is independent from the material originally used to derive it [also @barton2013]. Subsequent testing thereby provides an opportunity to reveal if one's accommodative belief is wrong.

Within a classic hypothetico-deductive system, an initial goal is to derive as many empirical implications of an explanation as possible. These implications are then to be tested by comparing these implications to actual observed data. Drawing on Carnap's [-@carnap1936, 425] \'gradually increasing confirmation\', this entails that each time a model matches the data, the confidence that the model is true is increased. If, on the other hand, the model fails, it can be discarded as untrue. This should thus lead to the continual rejection of false models, and move us ever closer to, but not necessarily to, the actual explanation of reality. Although certainly an enticing prospect, there are problems related to this approach irrespective of any goals of establishing general laws. 

A fundamental issue for hypothetico-deductivism, and scientific inference as a whole, follows from Hume's problem of induction [@ladyman2002, 31--61]. As an empiricist, all knowledge about the world was for Hume derived from sensory perception. Any reasoning that extend beyond observation, past or present, is based on cause and effect. However, since we can never observe a causal connection between events, the conjoined occurrence of observations is all we have to draw on. As there is no logical necessity for regularity in patterns to hold beyond what we can observe, there is no logical foundation for inductive reasoning -- there is no logical connection between the observable and unobservable. While we might have observed the sun rise every day, there is no logical contradiction in believing it will not rise tomorrow. Hume held that while inductive reasoning will continue to be fundamental to science, and our every-day lives, it has no logical justification.  

Following from the problem of induction, an issue for hypothetico-deductivism pertains to the value of testing an hypothesis, and whether with successful tests of our belief in the hypothesis should increase. The logical empiricist attempts at working around the problem of induction was never successful and a move to stating our belief in hypotheses in probabilistic terms never dissolved this fundamental issue [central here is what is known as the paradoxes of confirmation, e.g. @sprenger2023]. One of the most influential contentions with the issue of testing is found with Popper and his concept of falsificationism. Popper, also a sceptic of induction, held that the problems of induction cannot be resolved, but this is not of concern as science in fact progresses not with confirmation but with falsification. In an attempt at demarcating science from non-science, Popper [e.g. @popper] stated that a theory can only be considered scientific if it has the potential to be refuted by observation. A theory that is compatible with all empirical variation is unscientific. The test of an hypothesis should be aimed at falsifying it, not confirming it, and an hypothesis that is not proven false should simply be subjected to even more stringent and elaborate tests. It is with each new rejection of an hypothesis that science progresses and we learn something about the world. Although it will inevitably be falsified, a good theory for Popper is therefore one that is bold, risky and corresponds with the world in surprising ways. There are, however, further issues related to the prospect of confronting our beliefs with data.  

As insight from complex systems theory demonstrates, sensitivity to initial conditions can lead both different causes to produce similar results, and similar causes to produce different results (van der Leeuw 2004:121; Premo 2010). This reflects the problems of equifinality and underdetermination, as presented above, where several models can agree on the empirical data, but disagree on the underlying causal mechanisms. This follows from the ubiquity of measuring error and the sensitivity of complex systems to minute variation. One classic example in this regard is the weather, which can only be reliably predicted a few days into the future. Human systems are far more complex than that of the weather. Consequently, this renders the prospects of empirical confirmation or falsifiability weakened, and preference among different, even contradictory explanations, can often not hope to be based on observable data. In the case of archaeology, explanatory models are additionally faced with our generalisations of an already sparse and fragmented archaeological record, further increasing the likelihood that several explanations account equally well for the data at hand. However, this sensitivity to initial conditions can also impact the assumptions underlying an explanation. To show how this is an issue we can draw on what is known as the Duhem problem [after @duhem1906], which states that nothing is necessarily learned from rejecting an hypothesis on the grounds of a test.

Drawing on @hvidsten[2014, 184--187], we may postulate a simple model holding that mechanism A, under assumption B, implies C. If in a test we can reliably measure whether or not C is true, it would in a hypothetico-deductive understanding increase our belief in the model if C is true. In the case of Popper, the model is yet to be falsified. If, on the other hand, C is not true this would imply that either A or B are untrue. We would not, however, be able to derive logically which of A and B are untrue. This would perhaps not appear to be an immediate reason for concern. As long as one aspect of the model is untrue, the model is untrue, and should be rejected. The problem is that we know that models always contain a multitude of untrue assumptions. Drawing on the classic quote from Box above and the earlier discussion on abstraction, all models involve subsuming the virtual infinite complexity of reality and thus cannot work without an equal amount of untrue assumptions that could impact a test. In exemplifying the Duhem problem, @ladyman2002[77--78] gives the example of testing Newtonian gravitational theory by observing the travel of a comet. The theory of gravity alone does not provide a prediction for this path. It also depends on factors such as the mass of the comet, the mass of other objects in the solar system, and their relative positions, velocities and initial positions, as well as Newton's other laws of motion. If the test was to fail, this failure can follow from an untrue hypothesis, but also from a misspecification of an assumption that is subsumed in the test -- such as background conditions, measurement error, and initial conditions of the system. At some level a decision of whether the explanation has in fact been interfaced with observation is needed. As stated by @ladyman2002[80], \'falsification is only possible in science if there is intersubjective agreement among scientists about what is being tested.\' While a severely complicating issue for falsificationism, as Popper also recognised, his proposition still holds if it is qualified by stating that for a hypothesis to be scientific, it has to have the potential to be refuted by some kind of observation -- it has to risk exposure to observation [@godfrey-smith2003, 66]. The challenge then is determining what kind of observations this is.

Drawing on this issue with testing and falsificationism, several authors have argued that these are not the factors that determine the progression of science. This is known as a naturalistic perspective on science, which is concerned with understanding by precisely what processes science has arrived at its current beliefs about the world. While attempts at establishing formalistic recipies for undertaking research can offer important insights on what can constitute good components of strategies for scientific inquiry, such as Poppers falsificiationism, the scientific undertaking has been argued to be a far more messy enterprise. For example, Galilei's work to establish the Copernican model of the solar system is widely held as a classic example of the success of science, and has often been used to illustrate the text-book understanding of the scientific method in which one moves from making an observation, establishing an hypothesis, testing it against data, and then rejecting or adjusting one's hypothesis before repeating the process. However, several historians of science have argued if this understanding of the scientific method was to be followed, the erroneous Aristotelian model of the solar system should in fact have been preferred by Galileo, given the evidence at hand at the time. On the grounds of data alone there would be no reason for Galileo to have given any credence to the Copernican model, and due to seemingly contradictory evidence he should have persisted in trying to prove this alternative explanation. Parallels can be found in archaeology. For example, when new dates that dramatically push back the earliest human occupation in the Americas have been presented over the recent years [e.g. @holen2017], these have often been met with scepticism as related to their veracity [@braje2017; @magnani2019]. That an earlier occupation in the Americas than c. appear to thus far have been falsified does not mean that this hypothesis should be abandoned. Furthermore, how convincing an explanation is also clearly depends on more than data, not least because data is more than a simple binary category that is either observed/not observed. What data is accepted and what it is understood to represent is in part dependent on a decision by the person who observes and the wider research community. What we observe should to some degree dictate what we believe about the world. However, the examples above demonstrate that stringent empirical positivism is untenable and is not in fact how scientific insight is achieved.

## Models and scientific realism

Arguments such as those above have in sum both rendered suspect the empirical positivist’s absolute demand and adherence to observable data, and presents a significant challenge to prospect of testing our beliefs about the world. Following from @levins1966, a defining element of modern MBA is consequently the realisation that models cannot at the same time maximise generality, realism and precision, which means models cannot and will never be constructed or evaluated purely on the grounds of observable data (Kohler & van der Leeuw 2007:7; Lake 2015:26). However, if we were to concede to the fact that all models are wrong, how can we ever trust model-based inference?

In a classical instrumental understanding, the goal of science should be the prediction of phenomena that matter, a view famously forwarded by @friedman Friedman (2008[1953]). Whether prediction is achieved through the use of models that build on true causal mechanisms is irrelevant. As long as the predictions of the model has a satisfactory correspondence with the empirical variation of interest, it is deemed a success. This view is therefore compatible with the constraining realisation that all models are wrong, both because the truth of postulated causal mechanisms in and of itself does not matter, and because of the resulting relaxed demand for accordance with total empirical variation -- degree of empirical adequacy determines the choice between models. Related views have also been advanced within archaeology. The most clear example can be found in the domain of archaeological locational (sometimes termed 'predictive') modelling, concerned with understanding where archaeological sites are located in the landscape. These studies have sometimes focused on identifying where sites are located in the present-day landscape, irrespective of past motivations, so as to potentially reduce costs of land-development, or to help guide archaeological surveys in large areas where a complete coverage of the landscape is not possible. The concern then is knowing where sites are and are not located, not why. However, one of the criticisms forwarded towards instrumentalism is that if the ultimate goal is manipulation of relevant variables for the improvement of society, this will depend on uncovering true causal mechanisms. While mere prediction depends on stable correlation, control necessitates causality (Hausman 1998:190). As Elster (2015:18) puts it, explanation demands causation, and causation can never be revealed solely through prediction (see also Gibbon 1989:49). Instrumentalism, therefore, can never hope to explain social phenomena. Of course, explanation does not necessarily have to be the main concern for archaeology. One could argue that academic interest in explanation should not be the guiding principle behind archaeological inquiry but rather, for example, that mitigating costs associated with land-development or assembling interesting narratives about the past are more important goals. My view, as stated in the introduction to the thesis, follows from a form of realist understanding of truth and that scientific inquiry is as a strategy by which we try to confront theoretical constructs with empirical observation, aimed at aligning our beliefs as reliably as possible with what is true [@godfrey-smith2003, 161].
	
Scientific realism entails the philosophical position that there exist real observable and unobservable entities and properties, and that claims concerning either dimension cannot be set apart (@hausman1998; @psillos1999; @gibbon1989; @wylie2002, 97--105]. The goal is to reveal these truths, where truth typically follows a commonsensical definition of being determined by what is the case, and not, for example, what we believe to be true or what is most beneficial [@ladyman2002, 157--158; see also @malnes2012, 19--30]. Regardless of whether or not it is possible to ever achieve, the goal of the realist is to reveal true, yet unobservable causal mechanisms that generate and shape the flux of observable phenomena. In a realist view, even the most careful empirical approach depends on theoretical assumptions that will determine what hypotheses are deemed relevant, thus also determining how empirical data is treated and evaluated against these [@wylie2002, 100]. This point was also central to the post-processual critique of processualism, where @hodder1984 argued that objective data is never tested against separate independent theories. These theories already underlie and determine how the archaeological material is recorded and defined. To the realist, however, the realisation that we might view the world differently does not take away from the belief that we inhabit a common reality that exists and is true independently of what we think about it [@godfrey-smith2003, 174]. No meaningful separation can therefore be made between the epistemic value of observables and unobservables, and both should be treated as literally existing.

The H-D framework in a sense sees every model as a truth-candidate. Models can for the realist instead be understood as analytical tools, the purpose of which is to provide a concrete representation of the researchers beliefs, used to isolate or create a closed and credible surrogate system where causal mechanisms are allowed to work without impediment from surrounding noise (see Sugden 2000; Cartwright 2009; Mäki 2009 for variations on this; Sugden 2009). This is very much in line with the model as envisaged by Kohler and van der Leeuw (2007), as outlined above. The aim, according to @cartwright2009, is to reveal the capacities and differential contributions of unimpeded causal effects within an idealised structure. However, this does not necessarily entail that the causal contribution is stable outside the surrogate system. In an open target system, the complex interplay of several causal mechanism can render the contribution from the modelled causal effects completely transformed, compared to their role in an idealised surrogate system (Gibbon 1989:150). Therefore, although stable correlations can point to the possible existence of a causal relationship, the relevance of the realist study of capacities, unlike positivist regularities, does not presuppose closed target systems (Groff 2004:12–16). Positivism entails a closed system with regular conjunctions between events, such that an event of type A is always followed by an event of type B [@gibbon, 149]. @cartwright2009 contends that even though the realist surrogate system is credible, in the sense that the mechanisms could conceivably occur and result in the phenomena in question, the system is almost always different from all real cases in ways that matter. Drawing on the oft-invoked *ceteris paribus* statement, all other things are typically not equal (cf. Cartwright 2003[1983]:44–47) -- all models are wrong. 

One solution to this issue could perhaps be to probe the model for sensitivity to changes in assumptions and omissions, but @cartwright2009 demonstrates that this is ultimately unsatisfactory. She exemplifies that if A and B imply C, A and D imply C, A and E imply C etc., can this mean that A and anything implies C? This relates to Hume's problem of induction, and the fact that even if we have observed. True causality can be understood as dependent on a counter-factual condition (e.g. Lewis 1973), i.e. if not A then not C. This echoes the earlier discussion on equifinality, and means that emulation can never determine whether true causal mechanisms have been revealed (cf. @lake2015, 23--24). 

The confrontation of model and data can therefore never avoid the problem of induction. The question of interest then is not whether the model is true or false in its entirety, but if the model resembles the world in the relevant dimensions, given its purpose [@clarke2007, 747]. When evaluating a model in this view, the concern is if it corresponds with the world in ways that matter. A classic example in the literature is subway maps. These are clearly extreme simplifications of the world, and confronting them with how the world actually looks would easily show that they are not true renditions of reality. However, given it's purpose, the subway map should instead be evaluated by the degree to which it helps commuters get from A to B. Furthermore, any tests of models has been argued to be best done by comparing them to the ability of substantive competing alternatives to fulfill the same purpose, and not just their negation, the null-model [@smith2015; @wylie2002; @perrault2019]. Pitching alternative models against each other will lead away from a pure search for corroborative evidence, and. Relating this to the subway map, one could then for example compare its ability to help commuters to that of the topographic map. All models are wrong, but given the problem at hand, the subway map is likely to be the most useful alternative. While there might still exists other ways of representing the subway system that would be better than the current subway maps, given the current models at hand, the subway map would be the model of choice. 

This is related to what has often 

While theoretical discussions in archaeology have often operated at extremes of positivism and relativism, where transitions between these theoretical stances "crises". However, @wylie and others [@fogelin2007] have argued that these perspectives have not informed how archaeology as in fact progressed.


Given the case where multiple models achieve empirical adequacy, that is, they predict the empirical pattern of concern equally well, realist have argued that inference to the best explanation is the best way forward and that this represents a way to handle the unavoidable problem of induction. 

## What are models?
“It is […] more appropriate to describe models than to attempt a hopelessly broad or hopelessly narrow definition for them” [@clarke1972, 2]

Following Clarke, no conclusive definition of models is sought here. The concept is notoriously difficult to pin down as its use varies across different disciplines, between various authors, and a multitude of classificatory schemas of model types have been proposed (e.g. Hausman 1992; Morrison & Morgan 1999a; Gilbert 2008:5; @godfrey-smith2009; @clarke2007). Nonetheless, some possible understandings of the term are presented below, mainly in order to set up an understanding of modelling in relation to the practice of archaeological inquiry and to understand what could demarcate model-based archaeology (MBA) from other forms of archaeological inference.


Godfrey-Smith's [-@godfrey-smith2009, 102] understanding of a model-based style of science sees this as a distinct approach, starting at remove from the phenomena, or target system, of interest. A model-based approach is in this understanding thus different from an analysis that starts by trying to describe and understand the system under study. Instead it begins with the exploration of a hypothetical, fictional model, simplified and largely independent of the target system, before this is ultimately interfaced with empirical data, involving thus 'a deliberate detour through fiction' [@godfrey-smith2009, 103]. As the modeller in this instance would be in control of all parts of the artificial model and their interactions, it is in the empirical confrontation that the model can yield the most fruitful results, either by capturing the empirical variation of interest or by allowing for an exploration and understanding of where it fails. 

Another variation on the understanding of models also sees them as constructions that have similarities with, but exist independently of the target systems that they are to represent [@kohler2007]. Models are constructions used to draw further inferences about the reality they are to represent. How I understand this conception to be different from Godfrey-Smith's understanding is that here the models need not initially be conceived of independently of the target system. Models are here construed on the basis of what mechanisms we believe shaped the observables available to us. What is studied directly is the model, in the hope that the mechanisms of the model that the researcher is interested in correspond with those of the target system. These are sometimes termed idealised or isolating models (Gilbert 2008; Frigg & Hartmann 2018), and entail the exaggeration of the characteristics of interest, the inclusion of boundary conditions or assumptions considered essential for the model to function, and the explicit or silent omission of aspects deemed unessential [@maki2009].

For all the ambiguities in the above account of what can be taken to constitute models, a common element is the view that they are constructed and explicit representations of our beliefs. Precisely this is also central to the contention that one of the most important aspect of model-based approaches follow from their explorative side (Hausman 1992:77; Aydinonat 2007; Premo 2010). This results both from the assembly process itself, and from subsequent probing and manipulation of the model (Morrison & Morgan 1999b). In the initial construction of a representation of theory and data, the researcher is forced to concretise their assumptions and beliefs. This will likely lead to the adjustment of inconsistencies, the discovery of additional theoretical implications or relevant empirical patterns, and increase the opportunity for explicit handling and reporting of uncertainty. Through stringent and explicit aggregation of model features, further theoretical and empirical consequences are also likely to be revealed. Thus, in its construction, the model will already have provided valuable insights, regardless of its future archaeological life-span. Even so-called caricature models that are wildly unrealistic, extreme distortions have been argued to generate such insights (Gibbard & Varian 1978). Following its construction, it has been argued that further insight can be achieved through direct manipulation of model parameters (Morrison & Morgan 1999b:32-35). This holds the potential of revealing additional causal propensities and limitations that are difficult to reveal by passive study of the model, and can reveal how sensitive it is to such adjustments (Premo 2010). The same effects are subsequently extended by any attempts at evaluating the correspondence between model and target system, and by the involvement of an audience that comments, criticises, dismisses or helps align model and target system (Mäki 2009). 

A realist understanding holds that unobservable theoretical constructs should be conceived of as true mechanisms and events, and that these should not be distinguished from observables. In a realist view, the progression of science is not achieved by a random and undirected trial-and-error search by exploring empirical patterns, nor is it achieved by rejecting relevant hypotheses only on the basis of empirical adequacy. Our beliefs concerning unobservables shape how we observe, order and confront theoretical constructs with empirical data.   


## Why model? 

# 
The last chapter laid out the analytical framework that was used when thinking about these issue. The last section. This was not drawn on directly in the papers, but form a good frame both for considering their contribution and for setting up some future avenues along wich these could be explored.

<!-- One of the benefits of a MBA is thus that it aims at being a concrete representation of our beliefs about the world and allows this to be confronted with the world. When processual or New Archaeology entered the scene in the 1960s [e.g. @binford1962] this was based on criticism of how archaeology had been traditionally conducted, the main issue of which was argued to be a naive positivist and inductivist approach to the archaeological material [e.g. @gibbon1989, 61--90]. It was argued that in traditional archaeology the recovery and ordering of archaeological material was seen as an end in and of itself, following from a tacit belief that an understanding of the past would simply emerge when the material became systematised and adequately extensive. This is argued to have resulted in an inferential framework where culture was conceived of as a mentally internalised and aggregated unified concept, consisting of a univariate whole of norms or ideas. The archaeological record was to reflect this one model for cultural variation. Furthermore, by conceiving of culture as related to the mental domain, and cultural material as related to the empirical domain, this created an ontological break between the two dimensions, leading to a state where many cultural aspects relating to for example social relations and beliefs was not seen as manifest in the archaeological material. The empirical material will in this processual understanding therefore offer a direct link back to this systemic whole, thereby also collapsing the ontological break 
This view is normally ascribed to the critical realism of Bashkar (2008[1975]; Groff 2004; see Gibbon 1989 for an early account from an archaeological perspective).
-->
